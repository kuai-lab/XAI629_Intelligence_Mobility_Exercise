{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376507a8",
   "metadata": {},
   "source": [
    "# AgentFormer\n",
    "\n",
    "This repo contains the official implementation of our paper:\n",
    "\n",
    "**AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting**  \n",
    "Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris Kitani  \n",
    "**ICCV 2021**\n",
    "\n",
    "[[website](https://www.ye-yuan.com/agentformer)]  \n",
    "[[paper](https://arxiv.org/abs/2103.14023)]\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Khrylx/khrylx.github.io/main/agentformer/data/overview.png\" width=\"70%\">\n",
    "\n",
    "---\n",
    "\n",
    "> ⚠️ **Important Note**  \n",
    "> Please follow the **official AgentFormer GitHub repository README** for  \n",
    "> - setting up the **Python virtual environment**, and  \n",
    "> - downloading and placing the **pretrained model weights**.  \n",
    ">  \n",
    "> This notebook assumes that the environment and checkpoints are prepared **exactly as described in the original repository**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efb39824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "# 반드시 repo root에서 실행 중이어야 함\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from data.dataloader import data_generator\n",
    "from utils.torch import *\n",
    "from utils.config import Config\n",
    "from model.model_lib import model_dict\n",
    "from utils.utils import prepare_seed, print_log, mkdir_if_missing\n",
    "from utils.utils import initialize_weights\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from collections import defaultdict\n",
    "from torch.nn.functional import *\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.overrides import has_torch_function, handle_torch_function\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import (\n",
    "    Module, ModuleList, Linear, Dropout, LayerNorm, Parameter\n",
    ")\n",
    "from torch.nn.functional import linear, softmax, dropout, pad\n",
    "from torch.nn.init import xavier_uniform_, xavier_normal_, constant_\n",
    "\n",
    "model_dict = {\n",
    "    'agentformer': AgentFormer,\n",
    "    'dlow': DLow\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8b3c6",
   "metadata": {},
   "source": [
    "## Distribution Utilities (Normal & Categorical)\n",
    "\n",
    "This module defines lightweight distribution wrappers used for latent variable modeling.\n",
    "\n",
    "---\n",
    "\n",
    "**Normal**\n",
    "- Diagonal Gaussian distribution parameterized by `(mu, logvar)` or a single concatenated tensor.\n",
    "- Supports reparameterized sampling (`rsample`) and KL divergence computation.\n",
    "- Commonly used for continuous latent variables.\n",
    "\n",
    "**Key methods**\n",
    "- `rsample()`: reparameterized sampling\n",
    "- `kl(p)`: KL divergence to another Normal or standard Normal\n",
    "- `mode()`: mean of the distribution\n",
    "\n",
    "---\n",
    "\n",
    "**Categorical**\n",
    "- Categorical distribution with optional Gumbel-Softmax relaxation.\n",
    "- Supports differentiable sampling via `RelaxedOneHotCategorical`.\n",
    "- Used for discrete latent variables.\n",
    "\n",
    "**Key methods**\n",
    "- `rsample()`: relaxed (Gumbel-Softmax) sampling\n",
    "- `kl(p)`: KL divergence to another Categorical\n",
    "- `mode()`: one-hot vector of the most probable class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01934b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import distributions as td\n",
    "\n",
    "class Normal:\n",
    "\n",
    "    def __init__(self, mu=None, logvar=None, params=None):\n",
    "        super().__init__()\n",
    "        if params is not None:\n",
    "            self.mu, self.logvar = torch.chunk(params, chunks=2, dim=-1)\n",
    "        else:\n",
    "            assert mu is not None\n",
    "            assert logvar is not None\n",
    "            self.mu = mu\n",
    "            self.logvar = logvar\n",
    "        self.sigma = torch.exp(0.5 * self.logvar)\n",
    "\n",
    "    def rsample(self):\n",
    "        eps = torch.randn_like(self.sigma)\n",
    "        return self.mu + eps * self.sigma\n",
    "\n",
    "    def sample(self):\n",
    "        return self.rsample()\n",
    "\n",
    "    def kl(self, p=None):\n",
    "        \"\"\" compute KL(q||p) \"\"\"\n",
    "        if p is None:\n",
    "            kl = -0.5 * (1 + self.logvar - self.mu.pow(2) - self.logvar.exp())\n",
    "        else:\n",
    "            term1 = (self.mu - p.mu) / (p.sigma + 1e-8)\n",
    "            term2 = self.sigma / (p.sigma + 1e-8)\n",
    "            kl = 0.5 * (term1 * term1 + term2 * term2) - 0.5 - torch.log(term2)\n",
    "        return kl\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mu\n",
    "\n",
    "\n",
    "class Categorical:\n",
    "\n",
    "    def __init__(self, probs=None, logits=None, temp=0.01):\n",
    "        super().__init__()\n",
    "        self.logits = logits\n",
    "        self.temp = temp\n",
    "        if probs is not None:\n",
    "            self.probs = probs\n",
    "        else:\n",
    "            assert logits is not None\n",
    "            self.probs = torch.softmax(logits, dim=-1)\n",
    "        self.dist = td.OneHotCategorical(self.probs)\n",
    "\n",
    "    def rsample(self):\n",
    "        relatex_dist = td.RelaxedOneHotCategorical(self.temp, self.probs)\n",
    "        return relatex_dist.rsample()\n",
    "\n",
    "    def sample(self):\n",
    "        return self.dist.sample()\n",
    "\n",
    "    def kl(self, p=None):\n",
    "        \"\"\" compute KL(q||p) \"\"\"\n",
    "        if p is None:\n",
    "            p = Categorical(logits=torch.zeros_like(self.probs))\n",
    "        kl = td.kl_divergence(self.dist, p.dist)\n",
    "        return kl\n",
    "\n",
    "    def mode(self):\n",
    "        argmax = self.probs.argmax(dim=-1)\n",
    "        one_hot = torch.zeros_like(self.probs)\n",
    "        one_hot.scatter_(1, argmax.unsqueeze(1), 1)\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5308f",
   "metadata": {},
   "source": [
    "## MapCNN Module\n",
    "\n",
    "**MapCNN** is a lightweight CNN-based encoder that converts a rasterized local map patch\n",
    "into a fixed-dimensional feature vector.  \n",
    "It is typically used to encode environmental context (e.g., road layout or navigable areas)\n",
    "for trajectory prediction models.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Input</p>\n",
    "\n",
    "- Tensor shape: **`(B, C, H, W)`**\n",
    "  - `B`: batch size  \n",
    "  - `C`: number of map channels (`map_channels`)  \n",
    "  - `H, W`: spatial resolution of the map patch (`patch_size`)\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Architecture</p>\n",
    "\n",
    "- A stack of 2D convolutional layers defined by:\n",
    "  - Hidden dimensions: `hdim`\n",
    "  - Kernel sizes: `kernels`\n",
    "  - Strides: `strides`\n",
    "- Each convolution layer is followed by **LeakyReLU** activation  \n",
    "  *(negative slope = 0.2)*.\n",
    "- The final convolutional feature map is **flattened** and passed through a\n",
    "  **fully connected layer**.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Output</p>\n",
    "\n",
    "- A fixed-length feature vector of dimension **`out_dim`** for each input map.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Implementation Notes</p>\n",
    "\n",
    "- The output size of the convolutional stack is computed dynamically using a dummy input.\n",
    "- This avoids manual feature-size calculation and supports flexible map resolutions.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Purpose</p>\n",
    "\n",
    "- To provide compact and learnable map representations that can be fused with\n",
    "  agent motion features or latent variables in downstream models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91fb2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapCNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        map_channels = cfg.get('map_channels', 3)\n",
    "        patch_size = cfg.get('patch_size', [100, 100])\n",
    "        hdim = cfg.get('hdim', [32, 32])\n",
    "        kernels = cfg.get('kernels', [3, 3])\n",
    "        strides = cfg.get('strides', [3, 3])\n",
    "        self.out_dim = out_dim = cfg.get('out_dim', 32)\n",
    "        self.input_size = input_size = (map_channels, patch_size[0], patch_size[1])\n",
    "        x_dummy = torch.randn(input_size).unsqueeze(0)\n",
    "\n",
    "        for i, _ in enumerate(hdim):\n",
    "            self.convs.append(nn.Conv2d(map_channels if i == 0 else hdim[i-1],\n",
    "                                        hdim[i], kernels[i],\n",
    "                                        stride=strides[i]))\n",
    "            x_dummy = self.convs[i](x_dummy)\n",
    "\n",
    "        self.fc = nn.Linear(x_dummy.numel(), out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = F.leaky_relu(conv(x), 0.2)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2088e2",
   "metadata": {},
   "source": [
    "## MapEncoder Module\n",
    "\n",
    "**MapEncoder** is a configurable map feature encoder that supports multiple backbone\n",
    "architectures through a unified interface.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Overview</p>\n",
    "\n",
    "- Encodes rasterized map inputs into fixed-dimensional feature vectors.\n",
    "- Supports both lightweight CNNs and ResNet-based backbones.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Encoder Options</p>\n",
    "\n",
    "- **`map_cnn`**: custom lightweight CNN encoder (`MapCNN`)\n",
    "- **`resnet18 / resnet34 / resnet50`**: ResNet backbones with Instance Normalization  \n",
    "  and a replaced final linear layer.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Forward</p>\n",
    "\n",
    "- Optionally normalizes input maps to **[-1, 1]**.\n",
    "- Applies the selected encoder followed by dropout.\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"font-size:16px; font-weight:600; margin-bottom:4px;\">Purpose</p>\n",
    "\n",
    "- Provides a unified and flexible map encoding module for trajectory prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "718b0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        model_id = cfg.get('model_id', 'map_cnn')\n",
    "        dropout = cfg.get('dropout', 0.0)\n",
    "        self.normalize = cfg.get('normalize', True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if model_id == 'map_cnn':\n",
    "            self.model = MapCNN(cfg)\n",
    "            self.out_dim = self.model.out_dim\n",
    "        elif 'resnet' in model_id:\n",
    "            model_dict = {\n",
    "                'resnet18': models.resnet18,\n",
    "                'resnet34': models.resnet34,\n",
    "                'resnet50': models.resnet50\n",
    "            }\n",
    "            self.out_dim = out_dim = cfg.get('out_dim', 32)\n",
    "            self.model = model_dict[model_id](pretrained=False, norm_layer=nn.InstanceNorm2d)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, out_dim)\n",
    "        else:\n",
    "            raise ValueError('unknown map encoder!')\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.normalize:\n",
    "            x = x * 2. - 1.\n",
    "        x = self.model(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447eff6a",
   "metadata": {},
   "source": [
    "## MLP Module\n",
    "\n",
    "A simple multi-layer perceptron used for feature transformation.\n",
    "\n",
    "- Consists of stacked fully connected layers.\n",
    "- Applies a fixed activation function after each layer.\n",
    "- The output dimension is defined by the last hidden layer.\n",
    "\n",
    "**Supported activations**\n",
    "- `tanh`, `relu`, `sigmoid`\n",
    "\n",
    "**Usage**\n",
    "- Commonly used for latent projections and intermediate feature encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e52fbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=(128, 128), activation='tanh'):\n",
    "        super().__init__()\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "\n",
    "        self.out_dim = hidden_dims[-1]\n",
    "        self.affine_layers = nn.ModuleList()\n",
    "        last_dim = input_dim\n",
    "        for nh in hidden_dims:\n",
    "            self.affine_layers.append(nn.Linear(last_dim, nh))\n",
    "            last_dim = nh\n",
    "\n",
    "        initialize_weights(self.affine_layers.modules())        \n",
    "\n",
    "    def forward(self, x):\n",
    "        for affine in self.affine_layers:\n",
    "            x = self.activation(affine(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b9e3c",
   "metadata": {},
   "source": [
    "## Agent-Aware Attention\n",
    "\n",
    "An extension of multi-head attention that explicitly distinguishes\n",
    "**self-agent interactions** from **inter-agent interactions**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key ideas**\n",
    "- Based on standard multi-head attention.\n",
    "- Supports an optional **Gaussian distance-based attention kernel**.\n",
    "- Separates attention for:\n",
    "  - interactions **within the same agent**\n",
    "  - interactions **across different agents**\n",
    "\n",
    "---\n",
    "\n",
    "**Agent-aware mechanism**\n",
    "- Computes standard attention for inter-agent pairs.\n",
    "- Replaces self-agent attention blocks with a dedicated self-attention branch.\n",
    "- Uses an agent-wise block mask to combine both attentions.\n",
    "\n",
    "---\n",
    "\n",
    "**Output**\n",
    "- Attention output: `(L, N, E)`\n",
    "- Optional attention weights averaged over heads.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose**\n",
    "- Enables structured reasoning over multiple agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d6af861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_aware_attention(query: Tensor,\n",
    "                          key: Tensor,\n",
    "                          value: Tensor,\n",
    "                          embed_dim_to_check: int,\n",
    "                          num_heads: int,\n",
    "                          in_proj_weight: Tensor,\n",
    "                          in_proj_bias: Tensor,\n",
    "                          bias_k: Optional[Tensor],\n",
    "                          bias_v: Optional[Tensor],\n",
    "                          add_zero_attn: bool,\n",
    "                          dropout_p: float,\n",
    "                          out_proj_weight: Tensor,\n",
    "                          out_proj_bias: Tensor,\n",
    "                          training: bool = True,\n",
    "                          key_padding_mask: Optional[Tensor] = None,\n",
    "                          need_weights: bool = True,\n",
    "                          attn_mask: Optional[Tensor] = None,\n",
    "                          use_separate_proj_weight: bool = False,\n",
    "                          q_proj_weight: Optional[Tensor] = None,\n",
    "                          k_proj_weight: Optional[Tensor] = None,\n",
    "                          v_proj_weight: Optional[Tensor] = None,\n",
    "                          static_k: Optional[Tensor] = None,\n",
    "                          static_v: Optional[Tensor] = None,\n",
    "                          gaussian_kernel = True,\n",
    "                          num_agent = 1,\n",
    "                          in_proj_weight_self = None,\n",
    "                          in_proj_bias_self = None\n",
    "                          ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n",
    "                    out_proj_weight, out_proj_bias)\n",
    "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
    "            return handle_torch_function(\n",
    "                multi_head_attention_forward, tens_ops, query, key, value,\n",
    "                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n",
    "                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n",
    "                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=use_separate_proj_weight,\n",
    "                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n",
    "                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if torch.equal(query, key) and torch.equal(key, value):\n",
    "            # self-attention\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "            if in_proj_weight_self is not None:\n",
    "                q_self, k_self = linear(query, in_proj_weight_self, in_proj_bias_self).chunk(2, dim=-1)\n",
    "\n",
    "        elif torch.equal(key, value):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "            if in_proj_weight_self is not None:\n",
    "                _w = in_proj_weight_self[:embed_dim, :]\n",
    "                _b = in_proj_bias_self[:embed_dim]\n",
    "                q_self = linear(query, _w, _b)\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _w = in_proj_weight_self[embed_dim:, :]\n",
    "                _b = in_proj_bias_self[embed_dim:]\n",
    "                k_self = linear(key, _w, _b)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    if not gaussian_kernel:\n",
    "        q = q * scaling       # remove scaling\n",
    "        if in_proj_weight_self is not None:\n",
    "            q_self = q_self * scaling       # remove scaling\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if in_proj_weight_self is not None:\n",
    "        q_self = q_self.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "        k_self = k_self.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    if gaussian_kernel:\n",
    "        qk = torch.bmm(q, k.transpose(1, 2))\n",
    "        q_n = q.pow(2).sum(dim=-1).unsqueeze(-1)\n",
    "        k_n = k.pow(2).sum(dim=-1).unsqueeze(1)\n",
    "        qk_dist = q_n + k_n - 2 * qk\n",
    "        attn_output_weights = qk_dist * scaling * 0.5\n",
    "    else:\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if in_proj_weight_self is not None:\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "            Agent-Aware Attention\n",
    "        ==================================\n",
    "        \"\"\"\n",
    "        attn_output_weights_inter = attn_output_weights\n",
    "        attn_weight_self_mask = torch.eye(num_agent).to(q.device)\n",
    "        attn_weight_self_mask = attn_weight_self_mask.repeat([attn_output_weights.shape[1] // num_agent, attn_output_weights.shape[2] // num_agent]).unsqueeze(0)\n",
    "        attn_output_weights_self = torch.bmm(q_self, k_self.transpose(1, 2))\n",
    "\n",
    "        attn_output_weights = attn_output_weights_inter * (1 - attn_weight_self_mask) + attn_output_weights_self * attn_weight_self_mask\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        attn_output_weights = softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "    else:\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "\n",
    "        attn_output_weights = softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26696894",
   "metadata": {},
   "source": [
    "## AgentAwareAttention Module\n",
    "\n",
    "A multi-head attention module extended with **agent-aware mechanisms**.\n",
    "\n",
    "---\n",
    "\n",
    "**Overview**\n",
    "- Built on standard multi-head attention.\n",
    "- Supports optional **Gaussian distance-based attention**.\n",
    "- Explicitly separates **self-agent attention** and **inter-agent attention**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key features**\n",
    "- Configurable number of heads and embedding dimension.\n",
    "- Optional separate projection for self-agent interactions (`sep_attn`).\n",
    "- Compatible with both shared and separate Q/K/V projections.\n",
    "\n",
    "---\n",
    "\n",
    "**Forward behavior**\n",
    "- Applies agent-aware attention via `agent_aware_attention`.\n",
    "- Returns attention output and optional attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose**\n",
    "- Enables structured attention over multi-agent sequences\n",
    "  while preserving agent-specific dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bacdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentAwareAttention(Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, cfg, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.gaussian_kernel = self.cfg.get('gaussian_kernel', False)\n",
    "        self.sep_attn = self.cfg.get('sep_attn', True)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = Linear(embed_dim, embed_dim)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        if self.sep_attn:\n",
    "            self.in_proj_weight_self = Parameter(torch.empty(2 * embed_dim, embed_dim))\n",
    "            self.in_proj_bias_self = Parameter(torch.empty(2 * embed_dim))\n",
    "        else:\n",
    "            self.in_proj_weight_self = self.in_proj_bias_self = None\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "        if self.sep_attn:\n",
    "            xavier_uniform_(self.in_proj_weight_self)\n",
    "            constant_(self.in_proj_bias_self, 0.)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None, num_agent=1):\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return agent_aware_attention(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight, gaussian_kernel=self.gaussian_kernel,\n",
    "                num_agent=num_agent,\n",
    "                in_proj_weight_self=self.in_proj_weight_self,\n",
    "                in_proj_bias_self=self.in_proj_bias_self\n",
    "                )\n",
    "        else:\n",
    "            return agent_aware_attention(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, gaussian_kernel=self.gaussian_kernel,\n",
    "                num_agent=num_agent,\n",
    "                in_proj_weight_self=self.in_proj_weight_self,\n",
    "                in_proj_bias_self=self.in_proj_bias_self\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77324135",
   "metadata": {},
   "source": [
    "## AgentFormer Encoder & Decoder\n",
    "\n",
    "Transformer encoder–decoder architecture built with **agent-aware attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### Encoder Layer\n",
    "- Consists of:\n",
    "  - Agent-aware self-attention\n",
    "  - Feed-forward network\n",
    "- Uses residual connections and layer normalization.\n",
    "- Models interactions **within and across agents** in the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### Decoder Layer\n",
    "- Consists of:\n",
    "  - Agent-aware self-attention\n",
    "  - Agent-aware cross-attention to encoder outputs\n",
    "  - Feed-forward network\n",
    "- Returns optional self- and cross-attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "### Encoder / Decoder Stacks\n",
    "- Encoder and decoder are built by stacking multiple layers.\n",
    "- Optional final layer normalization.\n",
    "- Supports multi-agent sequences via `num_agent`.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "- Enables structured Transformer reasoning for **multi-agent trajectory modeling**,\n",
    "  capturing both agent-specific dynamics and inter-agent interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02ab86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentFormerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.self_attn = AgentAwareAttention(cfg, d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, num_agent=1) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask, num_agent=num_agent)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "class AgentFormerDecoderLayer(Module):\n",
    "    r\"\"\"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = decoder_layer(tgt, memory)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.self_attn = AgentAwareAttention(cfg, d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = AgentAwareAttention(cfg, d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.dropout3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None, num_agent = 1, need_weights = False) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        tgt2, self_attn_weights = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask, num_agent=num_agent, need_weights=need_weights)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, cross_attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask, num_agent=num_agent, need_weights=need_weights)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, self_attn_weights, cross_attn_weights\n",
    "\n",
    "\n",
    "class AgentFormerEncoder(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = transformer_encoder(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, num_agent=1) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, num_agent=num_agent)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class AgentFormerDecoder(Module):\n",
    "    r\"\"\"TransformerDecoder is a stack of N decoder layers\n",
    "\n",
    "    Args:\n",
    "        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n",
    "        num_layers: the number of sub-decoder-layers in the decoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = transformer_decoder(tgt, memory)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None, num_agent=1, need_weights = False) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "\n",
    "        self_attn_weights = [None] * len(self.layers)\n",
    "        cross_attn_weights = [None] * len(self.layers)\n",
    "        for i, mod in enumerate(self.layers):\n",
    "            output, self_attn_weights[i], cross_attn_weights[i] = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask,\n",
    "                         num_agent=num_agent, need_weights=need_weights)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        if need_weights:\n",
    "            self_attn_weights = torch.stack(self_attn_weights).cpu().numpy()\n",
    "            cross_attn_weights = torch.stack(cross_attn_weights).cpu().numpy()\n",
    "\n",
    "        return output, {'self_attn_weights': self_attn_weights, 'cross_attn_weights': cross_attn_weights}\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599f220",
   "metadata": {},
   "source": [
    "## AgentFormer Encoder & Decoder (Stacks)\n",
    "\n",
    "Lightweight wrappers that stack multiple agent-aware Transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "**Encoder**\n",
    "- Sequentially applies encoder layers.\n",
    "- Each layer models agent-aware self-attention.\n",
    "- Optional final layer normalization.\n",
    "\n",
    "---\n",
    "\n",
    "**Decoder**\n",
    "- Sequentially applies decoder layers.\n",
    "- Each layer performs:\n",
    "  - agent-aware self-attention\n",
    "  - agent-aware cross-attention to encoder outputs\n",
    "- Optionally returns attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose**\n",
    "- Provides scalable Transformer blocks for multi-agent sequence modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fd44c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentFormerEncoder(Module):\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None, num_agent=1) -> Tensor:\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, num_agent=num_agent)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AgentFormerDecoder(Module):\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "                tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                num_agent=1, need_weights=False):\n",
    "        output = tgt\n",
    "\n",
    "        self_attn_weights = [None] * len(self.layers)\n",
    "        cross_attn_weights = [None] * len(self.layers)\n",
    "\n",
    "        for i, mod in enumerate(self.layers):\n",
    "            output, self_attn_weights[i], cross_attn_weights[i] = mod(\n",
    "                output, memory,\n",
    "                tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "                num_agent=num_agent, need_weights=need_weights\n",
    "            )\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        if need_weights:\n",
    "            self_attn_weights = torch.stack(self_attn_weights).cpu().numpy()\n",
    "            cross_attn_weights = torch.stack(cross_attn_weights).cpu().numpy()\n",
    "\n",
    "        return output, {\n",
    "            'self_attn_weights': self_attn_weights,\n",
    "            'cross_attn_weights': cross_attn_weights\n",
    "        }\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39a1ac",
   "metadata": {},
   "source": [
    "## Masks, Positional Encoding, and Trajectory Modules\n",
    "\n",
    "Core utilities and modules for **agent-aware temporal modeling** in AgentFormer.\n",
    "\n",
    "---\n",
    "\n",
    "### Mask Generation\n",
    "- `generate_mask`: builds agent-aware attention masks between source and target sequences.\n",
    "- `generate_ar_mask`: enforces **autoregressive masking** across time while respecting agent boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### PositionalAgentEncoding\n",
    "- Adds **temporal positional encoding** and optional **agent identity encoding**.\n",
    "- Supports:\n",
    "  - additive encoding\n",
    "  - concatenation followed by linear projection\n",
    "- Enables disentangling **time** and **agent** information.\n",
    "\n",
    "---\n",
    "\n",
    "### ContextEncoder (Past Encoder)\n",
    "- Encodes observed trajectories into contextual representations.\n",
    "- Steps:\n",
    "  - Concatenate motion-related inputs (pos, vel, map, heading, etc.)\n",
    "  - Project to model dimension\n",
    "  - Apply positional + agent encoding\n",
    "  - Process with agent-aware Transformer encoder\n",
    "- Outputs:\n",
    "  - `context_enc`: per-timestep context\n",
    "  - `agent_context`: pooled per-agent representation\n",
    "\n",
    "---\n",
    "\n",
    "### FutureEncoder\n",
    "- Encodes future trajectories to infer latent variables `z`.\n",
    "- Uses agent-aware Transformer decoder over past context.\n",
    "- Produces:\n",
    "  - posterior distribution `q(z | past, future)`\n",
    "  - sampled latent `z`\n",
    "\n",
    "---\n",
    "\n",
    "### FutureDecoder\n",
    "- Autoregressively decodes future trajectories conditioned on:\n",
    "  - past context\n",
    "  - latent variable `z`\n",
    "- Supports different prediction spaces (position, velocity, normalized space).\n",
    "- Optionally learns a prior `p(z | past)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "- Together, these components form a **latent-variable, agent-aware Transformer**\n",
    "  for multi-agent trajectory prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a16e09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ar_mask(sz, agent_num, agent_mask):\n",
    "    assert sz % agent_num == 0\n",
    "    T = sz // agent_num\n",
    "    mask = agent_mask.repeat(T, T)\n",
    "    for t in range(T-1):\n",
    "        i1 = t * agent_num\n",
    "        i2 = (t+1) * agent_num\n",
    "        mask[i1:i2, i2:] = float('-inf')\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_mask(tgt_sz, src_sz, agent_num, agent_mask):\n",
    "    assert tgt_sz % agent_num == 0 and src_sz % agent_num == 0\n",
    "    mask = agent_mask.repeat(tgt_sz // agent_num, src_sz // agent_num)\n",
    "    return mask\n",
    "\n",
    "\n",
    "\"\"\" Positional Encoding \"\"\"\n",
    "class PositionalAgentEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_t_len=200, max_a_len=200, concat=False, use_agent_enc=False, agent_enc_learn=False):\n",
    "        super(PositionalAgentEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.concat = concat\n",
    "        self.d_model = d_model\n",
    "        self.use_agent_enc = use_agent_enc\n",
    "        if concat:\n",
    "            self.fc = nn.Linear((3 if use_agent_enc else 2) * d_model, d_model)\n",
    "\n",
    "        pe = self.build_pos_enc(max_t_len)\n",
    "        self.register_buffer('pe', pe)\n",
    "        if use_agent_enc:\n",
    "            if agent_enc_learn:\n",
    "                self.ae = nn.Parameter(torch.randn(max_a_len, 1, d_model) * 0.1)\n",
    "            else:\n",
    "                ae = self.build_pos_enc(max_a_len)\n",
    "                self.register_buffer('ae', ae)\n",
    "\n",
    "    def build_pos_enc(self, max_len):\n",
    "        pe = torch.zeros(max_len, self.d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        return pe\n",
    "\n",
    "    def build_agent_enc(self, max_len):\n",
    "        ae = torch.zeros(max_len, self.d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
    "        ae[:, 0::2] = torch.sin(position * div_term)\n",
    "        ae[:, 1::2] = torch.cos(position * div_term)\n",
    "        ae = ae.unsqueeze(0).transpose(0, 1)\n",
    "        return ae\n",
    "    \n",
    "    def get_pos_enc(self, num_t, num_a, t_offset):\n",
    "        pe = self.pe[t_offset: num_t + t_offset, :]\n",
    "        pe = pe.repeat_interleave(num_a, dim=0)\n",
    "        return pe\n",
    "\n",
    "    def get_agent_enc(self, num_t, num_a, a_offset, agent_enc_shuffle):\n",
    "        if agent_enc_shuffle is None:\n",
    "            ae = self.ae[a_offset: num_a + a_offset, :]\n",
    "        else:\n",
    "            ae = self.ae[agent_enc_shuffle]\n",
    "        ae = ae.repeat(num_t, 1, 1)\n",
    "        return ae\n",
    "\n",
    "    def forward(self, x, num_a, agent_enc_shuffle=None, t_offset=0, a_offset=0):\n",
    "        num_t = x.shape[0] // num_a\n",
    "        pos_enc = self.get_pos_enc(num_t, num_a, t_offset)\n",
    "        if self.use_agent_enc:\n",
    "            agent_enc = self.get_agent_enc(num_t, num_a, a_offset, agent_enc_shuffle)\n",
    "        if self.concat:\n",
    "            feat = [x, pos_enc.repeat(1, x.size(1), 1)]\n",
    "            if self.use_agent_enc:\n",
    "                feat.append(agent_enc.repeat(1, x.size(1), 1))\n",
    "            x = torch.cat(feat, dim=-1)\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x += pos_enc\n",
    "            if self.use_agent_enc:\n",
    "                x += agent_enc\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\"\"\" Context (Past) Encoder \"\"\"\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, cfg, ctx, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ctx = ctx\n",
    "        self.motion_dim = ctx['motion_dim']\n",
    "        self.model_dim = ctx['tf_model_dim']\n",
    "        self.ff_dim = ctx['tf_ff_dim']\n",
    "        self.nhead = ctx['tf_nhead']\n",
    "        self.dropout = ctx['tf_dropout']\n",
    "        self.nlayer = cfg.get('nlayer', 6)\n",
    "        self.input_type = ctx['input_type']\n",
    "        self.pooling = cfg.get('pooling', 'mean')\n",
    "        self.agent_enc_shuffle = ctx['agent_enc_shuffle']\n",
    "        self.vel_heading = ctx['vel_heading']\n",
    "        ctx['context_dim'] = self.model_dim\n",
    "        in_dim = self.motion_dim * len(self.input_type)\n",
    "        if 'map' in self.input_type:\n",
    "            in_dim += ctx['map_enc_dim'] - self.motion_dim\n",
    "        self.input_fc = nn.Linear(in_dim, self.model_dim)\n",
    "\n",
    "        encoder_layers = AgentFormerEncoderLayer(ctx['tf_cfg'], self.model_dim, self.nhead, self.ff_dim, self.dropout)\n",
    "        self.tf_encoder = AgentFormerEncoder(encoder_layers, self.nlayer)\n",
    "        self.pos_encoder = PositionalAgentEncoding(self.model_dim, self.dropout, concat=ctx['pos_concat'], max_a_len=ctx['max_agent_len'], use_agent_enc=ctx['use_agent_enc'], agent_enc_learn=ctx['agent_enc_learn'])\n",
    "\n",
    "    def forward(self, data):\n",
    "        traj_in = []\n",
    "        for key in self.input_type:\n",
    "            if key == 'pos':\n",
    "                traj_in.append(data['pre_motion'])\n",
    "            elif key == 'vel':\n",
    "                vel = data['pre_vel']\n",
    "                if len(self.input_type) > 1:\n",
    "                    vel = torch.cat([vel[[0]], vel], dim=0)\n",
    "                if self.vel_heading:\n",
    "                    vel = rotation_2d_torch(vel, -data['heading'])[0]\n",
    "                traj_in.append(vel)\n",
    "            elif key == 'norm':\n",
    "                traj_in.append(data['pre_motion_norm'])\n",
    "            elif key == 'scene_norm':\n",
    "                traj_in.append(data['pre_motion_scene_norm'])\n",
    "            elif key == 'heading':\n",
    "                hv = data['heading_vec'].unsqueeze(0).repeat((data['pre_motion'].shape[0], 1, 1))\n",
    "                traj_in.append(hv)\n",
    "            elif key == 'map':\n",
    "                map_enc = data['map_enc'].unsqueeze(0).repeat((data['pre_motion'].shape[0], 1, 1))\n",
    "                traj_in.append(map_enc)\n",
    "            else:\n",
    "                raise ValueError('unknown input_type!')\n",
    "        traj_in = torch.cat(traj_in, dim=-1)\n",
    "        tf_in = self.input_fc(traj_in.view(-1, traj_in.shape[-1])).view(-1, 1, self.model_dim)\n",
    "        agent_enc_shuffle = data['agent_enc_shuffle'] if self.agent_enc_shuffle else None\n",
    "        tf_in_pos = self.pos_encoder(tf_in, num_a=data['agent_num'], agent_enc_shuffle=agent_enc_shuffle)\n",
    "        \n",
    "        src_agent_mask = data['agent_mask'].clone()\n",
    "        src_mask = generate_mask(tf_in.shape[0], tf_in.shape[0], data['agent_num'], src_agent_mask).to(tf_in.device)\n",
    "        \n",
    "        data['context_enc'] = self.tf_encoder(tf_in_pos, mask=src_mask, num_agent=data['agent_num'])\n",
    "        \n",
    "        context_rs = data['context_enc'].view(-1, data['agent_num'], self.model_dim)\n",
    "        # compute per agent context\n",
    "        if self.pooling == 'mean':\n",
    "            data['agent_context'] = torch.mean(context_rs, dim=0)\n",
    "        else:\n",
    "            data['agent_context'] = torch.max(context_rs, dim=0)[0]\n",
    "\n",
    "\n",
    "\"\"\" Future Encoder \"\"\"\n",
    "class FutureEncoder(nn.Module):\n",
    "    def __init__(self, cfg, ctx, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.context_dim = context_dim = ctx['context_dim']\n",
    "        self.forecast_dim = forecast_dim = ctx['forecast_dim']\n",
    "        self.nz = ctx['nz']\n",
    "        self.z_type = ctx['z_type']\n",
    "        self.z_tau_annealer = ctx.get('z_tau_annealer', None)\n",
    "        self.model_dim = ctx['tf_model_dim']\n",
    "        self.ff_dim = ctx['tf_ff_dim']\n",
    "        self.nhead = ctx['tf_nhead']\n",
    "        self.dropout = ctx['tf_dropout']\n",
    "        self.nlayer = cfg.get('nlayer', 6)\n",
    "        self.out_mlp_dim = cfg.get('out_mlp_dim', None)\n",
    "        self.input_type = ctx['fut_input_type']\n",
    "        self.pooling = cfg.get('pooling', 'mean')\n",
    "        self.agent_enc_shuffle = ctx['agent_enc_shuffle']\n",
    "        self.vel_heading = ctx['vel_heading']\n",
    "        # networks\n",
    "        in_dim = forecast_dim * len(self.input_type)\n",
    "        if 'map' in self.input_type:\n",
    "            in_dim += ctx['map_enc_dim'] - forecast_dim\n",
    "        self.input_fc = nn.Linear(in_dim, self.model_dim)\n",
    "\n",
    "        decoder_layers = AgentFormerDecoderLayer(ctx['tf_cfg'], self.model_dim, self.nhead, self.ff_dim, self.dropout)\n",
    "        self.tf_decoder = AgentFormerDecoder(decoder_layers, self.nlayer)\n",
    "\n",
    "        self.pos_encoder = PositionalAgentEncoding(self.model_dim, self.dropout, concat=ctx['pos_concat'], max_a_len=ctx['max_agent_len'], use_agent_enc=ctx['use_agent_enc'], agent_enc_learn=ctx['agent_enc_learn'])\n",
    "        num_dist_params = 2 * self.nz if self.z_type == 'gaussian' else self.nz     # either gaussian or discrete\n",
    "        if self.out_mlp_dim is None:\n",
    "            self.q_z_net = nn.Linear(self.model_dim, num_dist_params)\n",
    "        else:\n",
    "            self.out_mlp = MLP(self.model_dim, self.out_mlp_dim, 'relu')\n",
    "            self.q_z_net = nn.Linear(self.out_mlp.out_dim, num_dist_params)\n",
    "        # initialize\n",
    "        initialize_weights(self.q_z_net.modules())\n",
    "\n",
    "    def forward(self, data, reparam=True):\n",
    "        traj_in = []\n",
    "        for key in self.input_type:\n",
    "            if key == 'pos':\n",
    "                traj_in.append(data['fut_motion'])\n",
    "            elif key == 'vel':\n",
    "                vel = data['fut_vel']\n",
    "                if self.vel_heading:\n",
    "                    vel = rotation_2d_torch(vel, -data['heading'])[0]\n",
    "                traj_in.append(vel)\n",
    "            elif key == 'norm':\n",
    "                traj_in.append(data['fut_motion_norm'])\n",
    "            elif key == 'scene_norm':\n",
    "                traj_in.append(data['fut_motion_scene_norm'])\n",
    "            elif key == 'heading':\n",
    "                hv = data['heading_vec'].unsqueeze(0).repeat((data['fut_motion'].shape[0], 1, 1))\n",
    "                traj_in.append(hv)\n",
    "            elif key == 'map':\n",
    "                map_enc = data['map_enc'].unsqueeze(0).repeat((data['fut_motion'].shape[0], 1, 1))\n",
    "                traj_in.append(map_enc)\n",
    "            else:\n",
    "                raise ValueError('unknown input_type!')\n",
    "        traj_in = torch.cat(traj_in, dim=-1)\n",
    "        tf_in = self.input_fc(traj_in.view(-1, traj_in.shape[-1])).view(-1, 1, self.model_dim)\n",
    "        agent_enc_shuffle = data['agent_enc_shuffle'] if self.agent_enc_shuffle else None\n",
    "        tf_in_pos = self.pos_encoder(tf_in, num_a=data['agent_num'], agent_enc_shuffle=agent_enc_shuffle)\n",
    "\n",
    "        mem_agent_mask = data['agent_mask'].clone()\n",
    "        tgt_agent_mask = data['agent_mask'].clone()\n",
    "        mem_mask = generate_mask(tf_in.shape[0], data['context_enc'].shape[0], data['agent_num'], mem_agent_mask).to(tf_in.device)\n",
    "        tgt_mask = generate_mask(tf_in.shape[0], tf_in.shape[0], data['agent_num'], tgt_agent_mask).to(tf_in.device)\n",
    "        \n",
    "        tf_out, _ = self.tf_decoder(tf_in_pos, data['context_enc'], memory_mask=mem_mask, tgt_mask=tgt_mask, num_agent=data['agent_num'])\n",
    "        tf_out = tf_out.view(traj_in.shape[0], -1, self.model_dim)\n",
    "\n",
    "        if self.pooling == 'mean':\n",
    "            h = torch.mean(tf_out, dim=0)\n",
    "        else:\n",
    "            h = torch.max(tf_out, dim=0)[0]\n",
    "        if self.out_mlp_dim is not None:\n",
    "            h = self.out_mlp(h)\n",
    "        q_z_params = self.q_z_net(h)\n",
    "        if self.z_type == 'gaussian':\n",
    "            data['q_z_dist'] = Normal(params=q_z_params)\n",
    "        else:\n",
    "            data['q_z_dist'] = Categorical(logits=q_z_params, temp=self.z_tau_annealer.val())\n",
    "        data['q_z_samp'] = data['q_z_dist'].rsample()\n",
    "\n",
    "\n",
    "\"\"\" Future Decoder \"\"\"\n",
    "class FutureDecoder(nn.Module):\n",
    "    def __init__(self, cfg, ctx, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ar_detach = ctx['ar_detach']\n",
    "        self.context_dim = context_dim = ctx['context_dim']\n",
    "        self.forecast_dim = forecast_dim = ctx['forecast_dim']\n",
    "        self.pred_scale = cfg.get('pred_scale', 1.0)\n",
    "        self.pred_type = ctx['pred_type']\n",
    "        self.sn_out_type = ctx['sn_out_type']\n",
    "        self.sn_out_heading = ctx['sn_out_heading']\n",
    "        self.input_type = ctx['dec_input_type']\n",
    "        self.future_frames = ctx['future_frames']\n",
    "        self.past_frames = ctx['past_frames']\n",
    "        self.nz = ctx['nz']\n",
    "        self.z_type = ctx['z_type']\n",
    "        self.model_dim = ctx['tf_model_dim']\n",
    "        self.ff_dim = ctx['tf_ff_dim']\n",
    "        self.nhead = ctx['tf_nhead']\n",
    "        self.dropout = ctx['tf_dropout']\n",
    "        self.nlayer = cfg.get('nlayer', 6)\n",
    "        self.out_mlp_dim = cfg.get('out_mlp_dim', None)\n",
    "        self.pos_offset = cfg.get('pos_offset', False)\n",
    "        self.agent_enc_shuffle = ctx['agent_enc_shuffle']\n",
    "        self.learn_prior = ctx['learn_prior']\n",
    "        # networks\n",
    "        in_dim = forecast_dim + len(self.input_type) * forecast_dim + self.nz\n",
    "        if 'map' in self.input_type:\n",
    "            in_dim += ctx['map_enc_dim'] - forecast_dim\n",
    "        self.input_fc = nn.Linear(in_dim, self.model_dim)\n",
    "\n",
    "        decoder_layers = AgentFormerDecoderLayer(ctx['tf_cfg'], self.model_dim, self.nhead, self.ff_dim, self.dropout)\n",
    "        self.tf_decoder = AgentFormerDecoder(decoder_layers, self.nlayer)\n",
    "\n",
    "        self.pos_encoder = PositionalAgentEncoding(self.model_dim, self.dropout, concat=ctx['pos_concat'], max_a_len=ctx['max_agent_len'], use_agent_enc=ctx['use_agent_enc'], agent_enc_learn=ctx['agent_enc_learn'])\n",
    "        if self.out_mlp_dim is None:\n",
    "            self.out_fc = nn.Linear(self.model_dim, forecast_dim)\n",
    "        else:\n",
    "            in_dim = self.model_dim\n",
    "            self.out_mlp = MLP(in_dim, self.out_mlp_dim, 'relu')\n",
    "            self.out_fc = nn.Linear(self.out_mlp.out_dim, forecast_dim)\n",
    "        initialize_weights(self.out_fc.modules())\n",
    "        if self.learn_prior:\n",
    "            num_dist_params = 2 * self.nz if self.z_type == 'gaussian' else self.nz     # either gaussian or discrete\n",
    "            self.p_z_net = nn.Linear(self.model_dim, num_dist_params)\n",
    "            initialize_weights(self.p_z_net.modules())\n",
    "\n",
    "    def decode_traj_ar(self, data, mode, context, pre_motion, pre_vel, pre_motion_scene_norm, z, sample_num, need_weights=False):\n",
    "        agent_num = data['agent_num']\n",
    "        if self.pred_type == 'vel':\n",
    "            dec_in = pre_vel[[-1]]\n",
    "        elif self.pred_type == 'pos':\n",
    "            dec_in = pre_motion[[-1]]\n",
    "        elif self.pred_type == 'scene_norm':\n",
    "            dec_in = pre_motion_scene_norm[[-1]]\n",
    "        else:\n",
    "            dec_in = torch.zeros_like(pre_motion[[-1]])\n",
    "        dec_in = dec_in.view(-1, sample_num, dec_in.shape[-1])\n",
    "        z_in = z.view(-1, sample_num, z.shape[-1])\n",
    "        in_arr = [dec_in, z_in]\n",
    "        for key in self.input_type:\n",
    "            if key == 'heading':\n",
    "                heading = data['heading_vec'].unsqueeze(1).repeat((1, sample_num, 1))\n",
    "                in_arr.append(heading)\n",
    "            elif key == 'map':\n",
    "                map_enc = data['map_enc'].unsqueeze(1).repeat((1, sample_num, 1))\n",
    "                in_arr.append(map_enc)\n",
    "            else:\n",
    "                raise ValueError('wrong decode input type!')\n",
    "        dec_in_z = torch.cat(in_arr, dim=-1)\n",
    "\n",
    "        mem_agent_mask = data['agent_mask'].clone()\n",
    "        tgt_agent_mask = data['agent_mask'].clone()\n",
    "\n",
    "        for i in range(self.future_frames):\n",
    "            tf_in = self.input_fc(dec_in_z.view(-1, dec_in_z.shape[-1])).view(dec_in_z.shape[0], -1, self.model_dim)\n",
    "            agent_enc_shuffle = data['agent_enc_shuffle'] if self.agent_enc_shuffle else None\n",
    "            tf_in_pos = self.pos_encoder(tf_in, num_a=agent_num, agent_enc_shuffle=agent_enc_shuffle, t_offset=self.past_frames-1 if self.pos_offset else 0)\n",
    "            # tf_in_pos = tf_in\n",
    "            mem_mask = generate_mask(tf_in.shape[0], context.shape[0], data['agent_num'], mem_agent_mask).to(tf_in.device)\n",
    "            tgt_mask = generate_ar_mask(tf_in_pos.shape[0], agent_num, tgt_agent_mask).to(tf_in.device)\n",
    "\n",
    "            tf_out, attn_weights = self.tf_decoder(tf_in_pos, context, memory_mask=mem_mask, tgt_mask=tgt_mask, num_agent=data['agent_num'], need_weights=need_weights)\n",
    "\n",
    "            out_tmp = tf_out.view(-1, tf_out.shape[-1])\n",
    "            if self.out_mlp_dim is not None:\n",
    "                out_tmp = self.out_mlp(out_tmp)\n",
    "            seq_out = self.out_fc(out_tmp).view(tf_out.shape[0], -1, self.forecast_dim)\n",
    "            if self.pred_type == 'scene_norm' and self.sn_out_type in {'vel', 'norm'}:\n",
    "                norm_motion = seq_out.view(-1, agent_num * sample_num, seq_out.shape[-1])\n",
    "                if self.sn_out_type == 'vel':\n",
    "                    norm_motion = torch.cumsum(norm_motion, dim=0)\n",
    "                if self.sn_out_heading:\n",
    "                    angles = data['heading'].repeat_interleave(sample_num)\n",
    "                    norm_motion = rotation_2d_torch(norm_motion, angles)[0]\n",
    "                seq_out = norm_motion + pre_motion_scene_norm[[-1]]\n",
    "                seq_out = seq_out.view(tf_out.shape[0], -1, seq_out.shape[-1])\n",
    "            if self.ar_detach:\n",
    "                out_in = seq_out[-agent_num:].clone().detach()\n",
    "            else:\n",
    "                out_in = seq_out[-agent_num:]\n",
    "            # create dec_in_z\n",
    "            in_arr = [out_in, z_in]\n",
    "            for key in self.input_type:\n",
    "                if key == 'heading':\n",
    "                    in_arr.append(heading)\n",
    "                elif key == 'map':\n",
    "                    in_arr.append(map_enc)\n",
    "                else:\n",
    "                    raise ValueError('wrong decoder input type!')\n",
    "            out_in_z = torch.cat(in_arr, dim=-1)\n",
    "            dec_in_z = torch.cat([dec_in_z, out_in_z], dim=0)\n",
    "\n",
    "        seq_out = seq_out.view(-1, agent_num * sample_num, seq_out.shape[-1])\n",
    "        data[f'{mode}_seq_out'] = seq_out\n",
    "\n",
    "        if self.pred_type == 'vel':\n",
    "            dec_motion = torch.cumsum(seq_out, dim=0)\n",
    "            dec_motion += pre_motion[[-1]]\n",
    "        elif self.pred_type == 'pos':\n",
    "            dec_motion = seq_out.clone()\n",
    "        elif self.pred_type == 'scene_norm':\n",
    "            dec_motion = seq_out + data['scene_orig']\n",
    "        else:\n",
    "            dec_motion = seq_out + pre_motion[[-1]]\n",
    "\n",
    "        dec_motion = dec_motion.transpose(0, 1).contiguous()       # M x frames x 7\n",
    "        if mode == 'infer':\n",
    "            dec_motion = dec_motion.view(-1, sample_num, *dec_motion.shape[1:])        # M x Samples x frames x 3\n",
    "        data[f'{mode}_dec_motion'] = dec_motion\n",
    "        if need_weights:\n",
    "            data['attn_weights'] = attn_weights\n",
    "\n",
    "    def decode_traj_batch(self, data, mode, context, pre_motion, pre_vel, pre_motion_scene_norm, z, sample_num):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, data, mode, sample_num=1, autoregress=True, z=None, need_weights=False):\n",
    "        context = data['context_enc'].repeat_interleave(sample_num, dim=1)       # 80 x 64\n",
    "        pre_motion = data['pre_motion'].repeat_interleave(sample_num, dim=1)             # 10 x 80 x 2\n",
    "        pre_vel = data['pre_vel'].repeat_interleave(sample_num, dim=1) if self.pred_type == 'vel' else None\n",
    "        pre_motion_scene_norm = data['pre_motion_scene_norm'].repeat_interleave(sample_num, dim=1)\n",
    "        \n",
    "        # p(z)\n",
    "        prior_key = 'p_z_dist' + ('_infer' if mode == 'infer' else '')\n",
    "        if self.learn_prior:\n",
    "            h = data['agent_context'].repeat_interleave(sample_num, dim=0)\n",
    "            p_z_params = self.p_z_net(h)\n",
    "            if self.z_type == 'gaussian':\n",
    "                data[prior_key] = Normal(params=p_z_params)\n",
    "            else:\n",
    "                data[prior_key] = Categorical(params=p_z_params)\n",
    "        else:\n",
    "            if self.z_type == 'gaussian':\n",
    "                data[prior_key] = Normal(mu=torch.zeros(pre_motion.shape[1], self.nz).to(pre_motion.device), logvar=torch.zeros(pre_motion.shape[1], self.nz).to(pre_motion.device))\n",
    "            else:\n",
    "                data[prior_key] = Categorical(logits=torch.zeros(pre_motion.shape[1], self.nz).to(pre_motion.device))\n",
    "\n",
    "        if z is None:\n",
    "            if mode in {'train', 'recon'}:\n",
    "                z = data['q_z_samp'] if mode == 'train' else data['q_z_dist'].mode()\n",
    "            elif mode == 'infer':\n",
    "                z = data['p_z_dist_infer'].sample()\n",
    "            else:\n",
    "                raise ValueError('Unknown Mode!')\n",
    "\n",
    "        if autoregress:\n",
    "            self.decode_traj_ar(data, mode, context, pre_motion, pre_vel, pre_motion_scene_norm, z, sample_num, need_weights=need_weights)\n",
    "        else:\n",
    "            self.decode_traj_batch(data, mode, context, pre_motion, pre_vel, pre_motion_scene_norm, z, sample_num)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea2ba8",
   "metadata": {},
   "source": [
    "## AgentFormer (Full Model)\n",
    "\n",
    "Top-level model that integrates **context encoding, latent modeling, and future decoding**\n",
    "for multi-agent trajectory prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Components\n",
    "- **ContextEncoder**: encodes past trajectories into agent-aware context.\n",
    "- **FutureEncoder**: infers latent variable `z` from future trajectories.\n",
    "- **FutureDecoder**: autoregressively predicts future trajectories conditioned on context and `z`.\n",
    "- **MapEncoder (optional)**: encodes local map patches per agent.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Processing\n",
    "- Prepares motion, velocity, normalization, and agent masks.\n",
    "- Supports scene normalization and optional random scene rotation.\n",
    "- Handles agent-wise masking and optional agent shuffling.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass\n",
    "1. Encode map (optional)\n",
    "2. Encode past context\n",
    "3. Infer latent variable `z`\n",
    "4. Decode future trajectories\n",
    "5. Optionally sample multiple futures\n",
    "\n",
    "---\n",
    "\n",
    "### Inference\n",
    "- Supports:\n",
    "  - reconstruction (`recon`)\n",
    "  - stochastic sampling (`infer`)\n",
    "- Outputs predicted trajectories and internal states.\n",
    "\n",
    "---\n",
    "\n",
    "### Training\n",
    "- Computes multiple loss terms (e.g., reconstruction, KL, diversity).\n",
    "- Supports latent annealing and autoregressive training.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "- Provides a unified **latent-variable, agent-aware Transformer**\n",
    "  for structured multi-agent trajectory forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f85bcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" AgentFormer \"\"\"\n",
    "class AgentFormer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        self.cfg = cfg\n",
    "\n",
    "        input_type = cfg.get('input_type', 'pos')\n",
    "        pred_type = cfg.get('pred_type', input_type)\n",
    "        if type(input_type) == str:\n",
    "            input_type = [input_type]\n",
    "        fut_input_type = cfg.get('fut_input_type', input_type)\n",
    "        dec_input_type = cfg.get('dec_input_type', [])\n",
    "        self.ctx = {\n",
    "            'tf_cfg': cfg.get('tf_cfg', {}),\n",
    "            'nz': cfg.nz,\n",
    "            'z_type': cfg.get('z_type', 'gaussian'),\n",
    "            'future_frames': cfg.future_frames,\n",
    "            'past_frames': cfg.past_frames,\n",
    "            'motion_dim': cfg.motion_dim,\n",
    "            'forecast_dim': cfg.forecast_dim,\n",
    "            'input_type': input_type,\n",
    "            'fut_input_type': fut_input_type,\n",
    "            'dec_input_type': dec_input_type,\n",
    "            'pred_type': pred_type,\n",
    "            'tf_nhead': cfg.tf_nhead,\n",
    "            'tf_model_dim': cfg.tf_model_dim,\n",
    "            'tf_ff_dim': cfg.tf_ff_dim,\n",
    "            'tf_dropout': cfg.tf_dropout,\n",
    "            'pos_concat': cfg.get('pos_concat', False),\n",
    "            'ar_detach': cfg.get('ar_detach', True),\n",
    "            'max_agent_len': cfg.get('max_agent_len', 128),\n",
    "            'use_agent_enc': cfg.get('use_agent_enc', False),\n",
    "            'agent_enc_learn': cfg.get('agent_enc_learn', False),\n",
    "            'agent_enc_shuffle': cfg.get('agent_enc_shuffle', False),\n",
    "            'sn_out_type': cfg.get('sn_out_type', 'scene_norm'),\n",
    "            'sn_out_heading': cfg.get('sn_out_heading', False),\n",
    "            'vel_heading': cfg.get('vel_heading', False),\n",
    "            'learn_prior': cfg.get('learn_prior', False),\n",
    "            'use_map': cfg.get('use_map', False)\n",
    "        }\n",
    "        self.use_map = self.ctx['use_map']\n",
    "        self.rand_rot_scene = cfg.get('rand_rot_scene', False)\n",
    "        self.discrete_rot = cfg.get('discrete_rot', False)\n",
    "        self.map_global_rot = cfg.get('map_global_rot', False)\n",
    "        self.ar_train = cfg.get('ar_train', True)\n",
    "        self.max_train_agent = cfg.get('max_train_agent', 100)\n",
    "        self.loss_cfg = self.cfg.loss_cfg\n",
    "        self.loss_names = list(self.loss_cfg.keys())\n",
    "        self.compute_sample = 'sample' in self.loss_names\n",
    "        self.param_annealers = nn.ModuleList()\n",
    "        if self.ctx['z_type'] == 'discrete':\n",
    "            self.ctx['z_tau_annealer'] = z_tau_annealer = ExpParamAnnealer(cfg.z_tau.start, cfg.z_tau.finish, cfg.z_tau.decay)\n",
    "            self.param_annealers.append(z_tau_annealer)\n",
    "\n",
    "        # save all computed variables\n",
    "        self.data = None\n",
    "        \n",
    "        # map encoder\n",
    "        if self.use_map:\n",
    "            self.map_encoder = MapEncoder(cfg.map_encoder)\n",
    "            self.ctx['map_enc_dim'] = self.map_encoder.out_dim\n",
    "\n",
    "        # models\n",
    "        self.context_encoder = ContextEncoder(cfg.context_encoder, self.ctx)\n",
    "        self.future_encoder = FutureEncoder(cfg.future_encoder, self.ctx)\n",
    "        self.future_decoder = FutureDecoder(cfg.future_decoder, self.ctx)\n",
    "        \n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def set_data(self, data):\n",
    "        device = self.device\n",
    "        if self.training and len(data['pre_motion_3D']) > self.max_train_agent:\n",
    "            in_data = {}\n",
    "            ind = np.random.choice(len(data['pre_motion_3D']), self.max_train_agent).tolist()\n",
    "            for key in ['pre_motion_3D', 'fut_motion_3D', 'fut_motion_mask', 'pre_motion_mask', 'heading']:\n",
    "                in_data[key] = [data[key][i] for i in ind if data[key] is not None]\n",
    "        else:\n",
    "            in_data = data\n",
    "\n",
    "        self.data = defaultdict(lambda: None)\n",
    "        self.data['batch_size'] = len(in_data['pre_motion_3D'])\n",
    "        self.data['agent_num'] = len(in_data['pre_motion_3D'])\n",
    "        self.data['pre_motion'] = torch.stack(in_data['pre_motion_3D'], dim=0).to(device).transpose(0, 1).contiguous()\n",
    "        self.data['fut_motion'] = torch.stack(in_data['fut_motion_3D'], dim=0).to(device).transpose(0, 1).contiguous()\n",
    "        self.data['fut_motion_orig'] = torch.stack(in_data['fut_motion_3D'], dim=0).to(device)   # future motion without transpose\n",
    "        self.data['fut_mask'] = torch.stack(in_data['fut_motion_mask'], dim=0).to(device)\n",
    "        self.data['pre_mask'] = torch.stack(in_data['pre_motion_mask'], dim=0).to(device)\n",
    "        scene_orig_all_past = self.cfg.get('scene_orig_all_past', False)\n",
    "        if scene_orig_all_past:\n",
    "            self.data['scene_orig'] = self.data['pre_motion'].view(-1, 2).mean(dim=0)\n",
    "        else:\n",
    "            self.data['scene_orig'] = self.data['pre_motion'][-1].mean(dim=0)\n",
    "        if in_data['heading'] is not None:\n",
    "            self.data['heading'] = torch.tensor(in_data['heading']).float().to(device)\n",
    "\n",
    "        # rotate the scene\n",
    "        if self.rand_rot_scene and self.training:\n",
    "            if self.discrete_rot:\n",
    "                theta = torch.randint(high=24, size=(1,)).to(device) * (np.pi / 12)\n",
    "            else:\n",
    "                theta = torch.rand(1).to(device) * np.pi * 2\n",
    "            for key in ['pre_motion', 'fut_motion', 'fut_motion_orig']:\n",
    "                self.data[f'{key}'], self.data[f'{key}_scene_norm'] = rotation_2d_torch(self.data[key], theta, self.data['scene_orig'])\n",
    "            if in_data['heading'] is not None:\n",
    "                self.data['heading'] += theta\n",
    "        else:\n",
    "            theta = torch.zeros(1).to(device)\n",
    "            for key in ['pre_motion', 'fut_motion', 'fut_motion_orig']:\n",
    "                self.data[f'{key}_scene_norm'] = self.data[key] - self.data['scene_orig']   # normalize per scene\n",
    "\n",
    "        self.data['pre_vel'] = self.data['pre_motion'][1:] - self.data['pre_motion'][:-1, :]\n",
    "        self.data['fut_vel'] = self.data['fut_motion'] - torch.cat([self.data['pre_motion'][[-1]], self.data['fut_motion'][:-1, :]])\n",
    "        self.data['cur_motion'] = self.data['pre_motion'][[-1]]\n",
    "        self.data['pre_motion_norm'] = self.data['pre_motion'][:-1] - self.data['cur_motion']   # normalize pos per agent\n",
    "        self.data['fut_motion_norm'] = self.data['fut_motion'] - self.data['cur_motion']\n",
    "        if in_data['heading'] is not None:\n",
    "            self.data['heading_vec'] = torch.stack([torch.cos(self.data['heading']), torch.sin(self.data['heading'])], dim=-1)\n",
    "\n",
    "        # agent maps\n",
    "        if self.use_map:\n",
    "            scene_map = data['scene_map']\n",
    "            scene_points = np.stack(in_data['pre_motion_3D'])[:, -1] * data['traj_scale']\n",
    "            if self.map_global_rot:\n",
    "                patch_size = [50, 50, 50, 50]\n",
    "                rot = theta.repeat(self.data['agent_num']).cpu().numpy() * (180 / np.pi)\n",
    "            else:\n",
    "                patch_size = [50, 10, 50, 90]\n",
    "                rot = -np.array(in_data['heading'])  * (180 / np.pi)\n",
    "            self.data['agent_maps'] = scene_map.get_cropped_maps(scene_points, patch_size, rot).to(device)\n",
    "\n",
    "        # agent shuffling\n",
    "        if self.training and self.ctx['agent_enc_shuffle']:\n",
    "            self.data['agent_enc_shuffle'] = torch.randperm(self.ctx['max_agent_len'])[:self.data['agent_num']].to(device)\n",
    "        else:\n",
    "            self.data['agent_enc_shuffle'] = None\n",
    "\n",
    "        conn_dist = self.cfg.get('conn_dist', 100000.0)\n",
    "        cur_motion = self.data['cur_motion'][0]\n",
    "        if conn_dist < 1000.0:\n",
    "            threshold = conn_dist / self.cfg.traj_scale\n",
    "            pdist = F.pdist(cur_motion)\n",
    "            D = torch.zeros([cur_motion.shape[0], cur_motion.shape[0]]).to(device)\n",
    "            D[np.triu_indices(cur_motion.shape[0], 1)] = pdist\n",
    "            D += D.T\n",
    "            mask = torch.zeros_like(D)\n",
    "            mask[D > threshold] = float('-inf')\n",
    "        else:\n",
    "            mask = torch.zeros([cur_motion.shape[0], cur_motion.shape[0]]).to(device)\n",
    "        self.data['agent_mask'] = mask\n",
    "\n",
    "    def step_annealer(self):\n",
    "        for anl in self.param_annealers:\n",
    "            anl.step()\n",
    "\n",
    "    def forward(self):\n",
    "        if self.use_map:\n",
    "            self.data['map_enc'] = self.map_encoder(self.data['agent_maps'])\n",
    "        self.context_encoder(self.data)\n",
    "        self.future_encoder(self.data)\n",
    "        self.future_decoder(self.data, mode='train', autoregress=self.ar_train)\n",
    "        if self.compute_sample:\n",
    "            self.inference(sample_num=self.loss_cfg['sample']['k'])\n",
    "        return self.data\n",
    "\n",
    "    def inference(self, mode='infer', sample_num=20, need_weights=False):\n",
    "        if self.use_map and self.data['map_enc'] is None:\n",
    "            self.data['map_enc'] = self.map_encoder(self.data['agent_maps'])\n",
    "        if self.data['context_enc'] is None:\n",
    "            self.context_encoder(self.data)\n",
    "        if mode == 'recon':\n",
    "            sample_num = 1\n",
    "            self.future_encoder(self.data)\n",
    "        self.future_decoder(self.data, mode=mode, sample_num=sample_num, autoregress=True, need_weights=need_weights)\n",
    "        return self.data[f'{mode}_dec_motion'], self.data\n",
    "\n",
    "    def compute_loss(self):\n",
    "        total_loss = 0\n",
    "        loss_dict = {}\n",
    "        loss_unweighted_dict = {}\n",
    "        for loss_name in self.loss_names:\n",
    "            loss, loss_unweighted = loss_func[loss_name](self.data, self.loss_cfg[loss_name])\n",
    "            total_loss += loss\n",
    "            loss_dict[loss_name] = loss.item()\n",
    "            loss_unweighted_dict[loss_name] = loss_unweighted.item()\n",
    "        return total_loss, loss_dict, loss_unweighted_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711a4c4",
   "metadata": {},
   "source": [
    "## DLow (Diversifying Latent Flows)\n",
    "\n",
    "DLow is a **latent diversification module** that generates **multiple diverse futures**\n",
    "by transforming a base predictor’s latent space.\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Functions\n",
    "- **KL Loss (`compute_z_kld`)**  \n",
    "  Aligns DLow latent distribution with the predictor’s prior.\n",
    "\n",
    "- **Diversity Loss (`diversity_loss`)**  \n",
    "  Encourages sampled trajectories to be different from each other.\n",
    "\n",
    "- **Reconstruction Loss (`recon_loss`)**  \n",
    "  Ensures at least one prediction matches the ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "- Learn an **affine transformation** of latent noise:\n",
    "  \n",
    "  \\[\n",
    "  z = A \\cdot \\epsilon + b\n",
    "  \\]\n",
    "\n",
    "- This produces **K diverse latent samples per agent**.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Structure\n",
    "- Uses a **pretrained prediction model** (AgentFormer).\n",
    "- Adds a small **Q-network (MLP)** to generate `(A, b)` for latent sampling.\n",
    "- Outputs a Gaussian latent distribution `q_z_dist_dlow`.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward / Inference\n",
    "1. Encode context using the pretrained model\n",
    "2. Sample (or use mean) latent variables\n",
    "3. Decode multiple future trajectories\n",
    "4. Optionally return attention weights\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "- Improves **trajectory diversity** without retraining the base predictor.\n",
    "- Enables **multi-modal future prediction** in a principled latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56c8e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_kld(data, cfg):\n",
    "    loss_unweighted = data['q_z_dist_dlow'].kl(data['p_z_dist_infer']).sum()\n",
    "    if cfg.get('normalize', True):\n",
    "        loss_unweighted /= data['batch_size']\n",
    "    loss_unweighted = loss_unweighted.clamp_min_(cfg.min_clip)\n",
    "    loss = loss_unweighted * cfg['weight']\n",
    "    return loss, loss_unweighted\n",
    "\n",
    "\n",
    "def diversity_loss(data, cfg):\n",
    "    loss_unweighted = 0\n",
    "    fut_motions = data['infer_dec_motion'].view(*data['infer_dec_motion'].shape[:2], -1)\n",
    "    for motion in fut_motions:\n",
    "        dist = F.pdist(motion, 2) ** 2\n",
    "        loss_unweighted += (-dist / cfg['d_scale']).exp().mean()\n",
    "    if cfg.get('normalize', True):\n",
    "        loss_unweighted /= data['batch_size']\n",
    "    loss = loss_unweighted * cfg['weight']\n",
    "    return loss, loss_unweighted\n",
    "\n",
    "\n",
    "def recon_loss(data, cfg):\n",
    "    diff = data['infer_dec_motion'] - data['fut_motion_orig'].unsqueeze(1)\n",
    "    if cfg.get('mask', True):\n",
    "        mask = data['fut_mask'].unsqueeze(1).unsqueeze(-1)\n",
    "        diff *= mask\n",
    "    dist = diff.pow(2).sum(dim=-1).sum(dim=-1)\n",
    "    loss_unweighted = dist.min(dim=1)[0]\n",
    "    if cfg.get('normalize', True):\n",
    "        loss_unweighted = loss_unweighted.mean()\n",
    "    else:\n",
    "        loss_unweighted = loss_unweighted.sum()\n",
    "    loss = loss_unweighted * cfg['weight']\n",
    "    return loss, loss_unweighted\n",
    "\n",
    "\n",
    "loss_func = {\n",
    "    'kld': compute_z_kld,\n",
    "    'diverse': diversity_loss,\n",
    "    'recon': recon_loss,\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\" DLow (Diversifying Latent Flows)\"\"\"\n",
    "class DLow(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        self.cfg = cfg\n",
    "        self.nk = nk = cfg.sample_k\n",
    "        self.nz = nz = cfg.nz\n",
    "        self.share_eps = cfg.get('share_eps', True)\n",
    "        self.train_w_mean = cfg.get('train_w_mean', False)\n",
    "        self.loss_cfg = self.cfg.loss_cfg\n",
    "        self.loss_names = list(self.loss_cfg.keys())\n",
    "\n",
    "        pred_cfg = Config(cfg.pred_cfg, tmp=False, create_dirs=False)\n",
    "        pred_model = model_dict[pred_cfg.model_id](pred_cfg)\n",
    "        self.pred_model_dim = pred_cfg.tf_model_dim\n",
    "        if cfg.pred_epoch > 0:\n",
    "            cp_path = pred_cfg.model_path % cfg.pred_epoch\n",
    "            print('loading model from checkpoint: %s' % cp_path)\n",
    "            model_cp = torch.load(cp_path, map_location='cpu')\n",
    "            pred_model.load_state_dict(model_cp['model_dict'])\n",
    "        pred_model.eval()\n",
    "        self.pred_model = [pred_model]\n",
    "\n",
    "        # Dlow's Q net\n",
    "        self.qnet_mlp = cfg.get('qnet_mlp', [512, 256])\n",
    "        self.q_mlp = MLP(self.pred_model_dim, self.qnet_mlp)\n",
    "        self.q_A = nn.Linear(self.q_mlp.out_dim, nk * nz)\n",
    "        self.q_b = nn.Linear(self.q_mlp.out_dim, nk * nz)\n",
    "        \n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        self.pred_model[0].set_device(device)\n",
    "\n",
    "    def set_data(self, data):\n",
    "        self.pred_model[0].set_data(data)\n",
    "        self.data = self.pred_model[0].data\n",
    "\n",
    "    def main(self, mean=False, need_weights=False):\n",
    "        pred_model = self.pred_model[0]\n",
    "        if hasattr(pred_model, 'use_map') and pred_model.use_map:\n",
    "            self.data['map_enc'] = pred_model.map_encoder(self.data['agent_maps'])\n",
    "        pred_model.context_encoder(self.data)\n",
    "\n",
    "        if not mean:\n",
    "            if self.share_eps:\n",
    "                eps = torch.randn([1, self.nz]).to(self.device)\n",
    "                eps = eps.repeat((self.data['agent_num'] * self.nk, 1))\n",
    "            else:\n",
    "                eps = torch.randn([self.data['agent_num'], self.nz]).to(self.device)\n",
    "                eps = eps.repeat_interleave(self.nk, dim=0)\n",
    "\n",
    "        qnet_h = self.q_mlp(self.data['agent_context'])\n",
    "        A = self.q_A(qnet_h).view(-1, self.nz)\n",
    "        b = self.q_b(qnet_h).view(-1, self.nz)\n",
    "\n",
    "        z = b if mean else A*eps + b\n",
    "        logvar = (A ** 2 + 1e-8).log()\n",
    "        self.data['q_z_dist_dlow'] = Normal(mu=b, logvar=logvar)\n",
    "\n",
    "        pred_model.future_decoder(self.data, mode='infer', sample_num=self.nk, autoregress=True, z=z, need_weights=need_weights)\n",
    "        return self.data\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.main(mean=self.train_w_mean)\n",
    "\n",
    "    def inference(self, mode, sample_num, need_weights=False):\n",
    "        self.main(mean=True, need_weights=need_weights)\n",
    "        res = self.data[f'infer_dec_motion']\n",
    "        if mode == 'recon':\n",
    "            res = res[:, 0]\n",
    "        return res, self.data\n",
    "\n",
    "    def compute_loss(self):\n",
    "        total_loss = 0\n",
    "        loss_dict = {}\n",
    "        loss_unweighted_dict = {}\n",
    "        for loss_name in self.loss_names:\n",
    "            loss, loss_unweighted = loss_func[loss_name](self.data, self.loss_cfg[loss_name])\n",
    "            total_loss += loss\n",
    "            loss_dict[loss_name] = loss.item()\n",
    "            loss_unweighted_dict[loss_name] = loss_unweighted.item()\n",
    "        return total_loss, loss_dict, loss_unweighted_dict\n",
    "\n",
    "    def step_annealer(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7978624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(data, sample_k):\n",
    "    model.set_data(data)\n",
    "    recon_motion_3D, _ = model.inference(mode='recon', sample_num=sample_k)\n",
    "    sample_motion_3D, data = model.inference(\n",
    "        mode='infer', sample_num=sample_k, need_weights=False\n",
    "    )\n",
    "    sample_motion_3D = sample_motion_3D.transpose(0, 1).contiguous()\n",
    "    return recon_motion_3D, sample_motion_3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d6fa439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(pred, data, suffix, save_dir):\n",
    "    pred_num = 0\n",
    "    pred_arr = []\n",
    "\n",
    "    fut_data = data['fut_data']\n",
    "    seq_name = data['seq']\n",
    "    frame = data['frame']\n",
    "    valid_id = data['valid_id']\n",
    "    pred_mask = data['pred_mask']\n",
    "\n",
    "    for i in range(len(valid_id)):\n",
    "        identity = valid_id[i]\n",
    "        if pred_mask is not None and pred_mask[i] != 1.0:\n",
    "            continue\n",
    "\n",
    "        for j in range(cfg.future_frames):\n",
    "            cur_data = fut_data[j]\n",
    "            if len(cur_data) > 0 and identity in cur_data[:, 1]:\n",
    "                cur = cur_data[cur_data[:, 1] == identity].squeeze()\n",
    "            else:\n",
    "                cur = most_recent_data.copy()\n",
    "                cur[0] = frame + j + 1\n",
    "\n",
    "            cur[[13, 15]] = pred[i, j].cpu().numpy()\n",
    "            most_recent_data = cur.copy()\n",
    "            pred_arr.append(cur)\n",
    "\n",
    "        pred_num += 1\n",
    "\n",
    "    if len(pred_arr) > 0:\n",
    "        pred_arr = np.vstack(pred_arr)\n",
    "        pred_arr = pred_arr[:, [0, 1, 13, 15]]  # frame, id, x, z\n",
    "\n",
    "        fname = f'{save_dir}/{seq_name}/frame_{int(frame):06d}{suffix}.txt'\n",
    "        mkdir_if_missing(fname)\n",
    "        np.savetxt(fname, pred_arr, fmt=\"%.3f\")\n",
    "\n",
    "    return pred_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b9c7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(generator, save_dir, cfg):\n",
    "    total_num_pred = 0\n",
    "\n",
    "    while not generator.is_epoch_end():\n",
    "        data = generator()\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        seq_name = data['seq']\n",
    "        frame = int(data['frame'])\n",
    "\n",
    "        print(f\"Testing seq: {seq_name}, frame: {frame:06d}\", end=\"\\r\")\n",
    "\n",
    "        gt_motion_3D = torch.stack(\n",
    "            data['fut_motion_3D'], dim=0\n",
    "        ).to(device) * cfg.traj_scale\n",
    "\n",
    "        with torch.no_grad():\n",
    "            recon_motion_3D, sample_motion_3D = get_model_prediction(\n",
    "                data, cfg.sample_k\n",
    "            )\n",
    "\n",
    "        recon_motion_3D *= cfg.traj_scale\n",
    "        sample_motion_3D *= cfg.traj_scale\n",
    "\n",
    "        recon_dir = os.path.join(save_dir, 'recon')\n",
    "        sample_dir = os.path.join(save_dir, 'samples')\n",
    "        gt_dir = os.path.join(save_dir, 'gt')\n",
    "\n",
    "        mkdir_if_missing(recon_dir)\n",
    "        mkdir_if_missing(sample_dir)\n",
    "        mkdir_if_missing(gt_dir)\n",
    "\n",
    "        for i in range(sample_motion_3D.shape[0]):\n",
    "            save_prediction(sample_motion_3D[i], data, f'/sample_{i:03d}', sample_dir)\n",
    "\n",
    "        save_prediction(recon_motion_3D, data, '', recon_dir)\n",
    "        num_pred = save_prediction(gt_motion_3D, data, '', gt_dir)\n",
    "\n",
    "        total_num_pred += num_pred\n",
    "\n",
    "    print_log(f'\\nTotal predictions: {total_num_pred}', log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg_id: eth_agentformer\n",
      "epochs: [5]\n",
      "device: cpu\n",
      "dataset: eth model_id: dlow\n",
      "model_path template: results/eth_agentformer/models/model_%04d.p\n",
      "result_dir: results/eth_agentformer/results\n",
      "loading model from checkpoint: results/eth_agentformer_pre/models/model_0030.p\n",
      "loading model from checkpoint: results/eth_agentformer/models/model_0005.p\n",
      "\n",
      "-------------------------- loading test data --------------------------\n",
      "loading sequence biwi_eth ...\n",
      "total num samples: 1142\n",
      "------------------------------ done --------------------------------\n",
      "\n",
      "Testing seq: biwi_eth, frame: 000112\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing seq: biwi_eth, frame: 001226\n",
      "Total predictions: 364\n",
      "RUN: python eval.py --dataset eth --results_dir results/eth_agentformer/results/epoch_0005/test/samples --data test --log results/eth_agentformer/log/log_eval.txt\n",
      "loading results from results/eth_agentformer/results/epoch_0005/test/samples\n",
      "loading GT from datasets/eth_ucy/eth\n",
      "\n",
      "\n",
      "number of sequences to evaluate is 1\n",
      "evaluating seq biwi_eth, forecasting frame 000088 to 000099 ADE: 0.7350 (0.7350) FDE: 0.5732 (0.5732)\n",
      "evaluating seq biwi_eth, forecasting frame 000089 to 000100 ADE: 0.9771 (0.8560) FDE: 1.1065 (0.8398)\n",
      "evaluating seq biwi_eth, forecasting frame 000090 to 000101 ADE: 0.6236 (0.7786) FDE: 0.2495 (0.6431)\n",
      "evaluating seq biwi_eth, forecasting frame 000091 to 000102 ADE: 0.8057 (0.7894) FDE: 0.4892 (0.5815)\n",
      "evaluating seq biwi_eth, forecasting frame 000113 to 000124 ADE: 0.2616 (0.6386) FDE: 0.5791 (0.5808)\n",
      "evaluating seq biwi_eth, forecasting frame 000294 to 000305 ADE: 0.2732 (0.5290) FDE: 0.4975 (0.5558)\n",
      "evaluating seq biwi_eth, forecasting frame 000295 to 000306 ADE: 0.1415 (0.4396) FDE: 0.1505 (0.4623)\n",
      "evaluating seq biwi_eth, forecasting frame 000296 to 000307 ADE: 0.1220 (0.3800) FDE: 0.2077 (0.4146)\n",
      "evaluating seq biwi_eth, forecasting frame 000297 to 000308 ADE: 0.1426 (0.3426) FDE: 0.2072 (0.3818)\n",
      "evaluating seq biwi_eth, forecasting frame 000298 to 000309 ADE: 0.1557 (0.3171) FDE: 0.1892 (0.3556)\n",
      "evaluating seq biwi_eth, forecasting frame 000299 to 000310 ADE: 0.1702 (0.2995) FDE: 0.1768 (0.3341)\n",
      "evaluating seq biwi_eth, forecasting frame 000300 to 000311 ADE: 0.1800 (0.2866) FDE: 0.1626 (0.3157)\n",
      "evaluating seq biwi_eth, forecasting frame 000301 to 000312 ADE: 0.2117 (0.2794) FDE: 0.2311 (0.3075)\n",
      "evaluating seq biwi_eth, forecasting frame 000302 to 000313 ADE: 0.2187 (0.2740) FDE: 0.3880 (0.3146)\n",
      "evaluating seq biwi_eth, forecasting frame 000303 to 000314 ADE: 0.2604 (0.2729) FDE: 0.4116 (0.3225)\n",
      "evaluating seq biwi_eth, forecasting frame 000304 to 000315 ADE: 0.3028 (0.2752) FDE: 0.6749 (0.3489)\n",
      "evaluating seq biwi_eth, forecasting frame 000305 to 000316 ADE: 0.3325 (0.2792) FDE: 0.5087 (0.3601)\n",
      "evaluating seq biwi_eth, forecasting frame 000306 to 000317 ADE: 0.3218 (0.2819) FDE: 0.4494 (0.3659)\n",
      "evaluating seq biwi_eth, forecasting frame 000307 to 000318 ADE: 0.3238 (0.2837) FDE: 0.3780 (0.3664)\n",
      "evaluating seq biwi_eth, forecasting frame 000308 to 000319 ADE: 0.2158 (0.2810) FDE: 0.2643 (0.3623)\n",
      "evaluating seq biwi_eth, forecasting frame 000309 to 000320 ADE: 0.2057 (0.2781) FDE: 0.1244 (0.3532)\n",
      "evaluating seq biwi_eth, forecasting frame 000310 to 000321 ADE: 0.2305 (0.2763) FDE: 0.1876 (0.3470)\n",
      "evaluating seq biwi_eth, forecasting frame 000311 to 000322 ADE: 0.2266 (0.2745) FDE: 0.2784 (0.3446)\n",
      "evaluating seq biwi_eth, forecasting frame 000312 to 000323 ADE: 0.3571 (0.2774) FDE: 1.0016 (0.3673)\n",
      "evaluating seq biwi_eth, forecasting frame 000313 to 000324 ADE: 0.3003 (0.2782) FDE: 0.2892 (0.3647)\n",
      "evaluating seq biwi_eth, forecasting frame 000441 to 000452 ADE: 0.4445 (0.2809) FDE: 0.7726 (0.3713)\n",
      "evaluating seq biwi_eth, forecasting frame 000650 to 000661 ADE: 0.4860 (0.2874) FDE: 1.0224 (0.3920)\n",
      "evaluating seq biwi_eth, forecasting frame 000820 to 000831 ADE: 0.5231 (0.2911) FDE: 0.5169 (0.3940)\n",
      "evaluating seq biwi_eth, forecasting frame 000821 to 000832 ADE: 0.4760 (0.2939) FDE: 0.7462 (0.3994)\n",
      "evaluating seq biwi_eth, forecasting frame 000822 to 000833 ADE: 0.7767 (0.3012) FDE: 0.8115 (0.4056)\n",
      "evaluating seq biwi_eth, forecasting frame 000823 to 000834 ADE: 0.4925 (0.3041) FDE: 0.8812 (0.4127)\n",
      "evaluating seq biwi_eth, forecasting frame 000824 to 000835 ADE: 0.3010 (0.3040) FDE: 0.3769 (0.4122)\n",
      "evaluating seq biwi_eth, forecasting frame 000825 to 000836 ADE: 0.4967 (0.3068) FDE: 0.5234 (0.4138)\n",
      "evaluating seq biwi_eth, forecasting frame 000826 to 000837 ADE: 0.6557 (0.3118) FDE: 0.7150 (0.4181)\n",
      "evaluating seq biwi_eth, forecasting frame 000827 to 000838 ADE: 0.7009 (0.3173) FDE: 0.6515 (0.4214)\n",
      "evaluating seq biwi_eth, forecasting frame 000828 to 000839 ADE: 0.5748 (0.3209) FDE: 0.6739 (0.4249)\n",
      "evaluating seq biwi_eth, forecasting frame 000829 to 000840 ADE: 0.3463 (0.3212) FDE: 0.9064 (0.4315)\n",
      "evaluating seq biwi_eth, forecasting frame 000830 to 000841 ADE: 0.6761 (0.3260) FDE: 1.3948 (0.4445)\n",
      "evaluating seq biwi_eth, forecasting frame 000831 to 000842 ADE: 0.4473 (0.3276) FDE: 1.1823 (0.4544)\n",
      "evaluating seq biwi_eth, forecasting frame 000832 to 000843 ADE: 0.3650 (0.3281) FDE: 0.6890 (0.4574)\n",
      "evaluating seq biwi_eth, forecasting frame 000833 to 000844 ADE: 0.5875 (0.3315) FDE: 1.1455 (0.4664)\n",
      "evaluating seq biwi_eth, forecasting frame 000834 to 000845 ADE: 0.4934 (0.3336) FDE: 0.9659 (0.4728)\n",
      "evaluating seq biwi_eth, forecasting frame 000835 to 000846 ADE: 0.4969 (0.3356) FDE: 0.5015 (0.4731)\n",
      "evaluating seq biwi_eth, forecasting frame 000836 to 000847 ADE: 0.6190 (0.3392) FDE: 0.5058 (0.4736)\n",
      "evaluating seq biwi_eth, forecasting frame 000837 to 000848 ADE: 0.7887 (0.3447) FDE: 0.5894 (0.4750)\n",
      "evaluating seq biwi_eth, forecasting frame 000838 to 000849 ADE: 0.4483 (0.3460) FDE: 0.5105 (0.4754)\n",
      "evaluating seq biwi_eth, forecasting frame 000839 to 000850 ADE: 0.2018 (0.3443) FDE: 0.1799 (0.4719)\n",
      "evaluating seq biwi_eth, forecasting frame 000840 to 000851 ADE: 0.4157 (0.3451) FDE: 0.3779 (0.4707)\n",
      "evaluating seq biwi_eth, forecasting frame 000841 to 000852 ADE: 0.3336 (0.3450) FDE: 0.6657 (0.4730)\n",
      "evaluating seq biwi_eth, forecasting frame 000842 to 000853 ADE: 0.4355 (0.3460) FDE: 0.8032 (0.4769)\n",
      "evaluating seq biwi_eth, forecasting frame 000843 to 000854 ADE: 0.1551 (0.3438) FDE: 0.2911 (0.4747)\n",
      "evaluating seq biwi_eth, forecasting frame 000844 to 000855 ADE: 0.1388 (0.3415) FDE: 0.1960 (0.4716)\n",
      "evaluating seq biwi_eth, forecasting frame 000845 to 000856 ADE: 0.2036 (0.3399) FDE: 0.3598 (0.4703)\n",
      "evaluating seq biwi_eth, forecasting frame 000846 to 000857 ADE: 0.2413 (0.3389) FDE: 0.4882 (0.4705)\n",
      "evaluating seq biwi_eth, forecasting frame 000847 to 000858 ADE: 0.2598 (0.3380) FDE: 0.6449 (0.4724)\n",
      "evaluating seq biwi_eth, forecasting frame 000848 to 000859 ADE: 0.2343 (0.3369) FDE: 0.4134 (0.4718)\n",
      "evaluating seq biwi_eth, forecasting frame 000849 to 000860 ADE: 0.2252 (0.3357) FDE: 0.6869 (0.4741)\n",
      "evaluating seq biwi_eth, forecasting frame 000850 to 000861 ADE: 0.2510 (0.3348) FDE: 0.5043 (0.4744)\n",
      "evaluating seq biwi_eth, forecasting frame 000851 to 000862 ADE: 0.2589 (0.3340) FDE: 0.4423 (0.4741)\n",
      "evaluating seq biwi_eth, forecasting frame 000852 to 000863 ADE: 0.2649 (0.3332) FDE: 0.7385 (0.4768)\n",
      "evaluating seq biwi_eth, forecasting frame 000853 to 000864 ADE: 0.4476 (0.3344) FDE: 1.1267 (0.4835)\n",
      "evaluating seq biwi_eth, forecasting frame 000854 to 000865 ADE: 0.8337 (0.3395) FDE: 1.5740 (0.4947)\n",
      "evaluating seq biwi_eth, forecasting frame 000855 to 000866 ADE: 1.2173 (0.3484) FDE: 2.0304 (0.5102)\n",
      "evaluating seq biwi_eth, forecasting frame 000856 to 000867 ADE: 0.4406 (0.3493) FDE: 0.3103 (0.5082)\n",
      "evaluating seq biwi_eth, forecasting frame 000857 to 000868 ADE: 1.2205 (0.3579) FDE: 2.1010 (0.5239)\n",
      "evaluating seq biwi_eth, forecasting frame 000858 to 000869 ADE: 0.9278 (0.3635) FDE: 1.3792 (0.5323)\n",
      "evaluating seq biwi_eth, forecasting frame 000859 to 000870 ADE: 0.2575 (0.3625) FDE: 0.4370 (0.5314)\n",
      "evaluating seq biwi_eth, forecasting frame 000860 to 000871 ADE: 0.2466 (0.3614) FDE: 0.3981 (0.5301)\n",
      "evaluating seq biwi_eth, forecasting frame 000861 to 000872 ADE: 0.3566 (0.3613) FDE: 0.4339 (0.5292)\n",
      "evaluating seq biwi_eth, forecasting frame 000862 to 000873 ADE: 0.1379 (0.3592) FDE: 0.3538 (0.5276)\n",
      "evaluating seq biwi_eth, forecasting frame 000863 to 000874 ADE: 0.4135 (0.3597) FDE: 0.8353 (0.5304)\n",
      "evaluating seq biwi_eth, forecasting frame 000864 to 000875 ADE: 0.1836 (0.3581) FDE: 0.0420 (0.5259)\n",
      "evaluating seq biwi_eth, forecasting frame 000865 to 000876 ADE: 0.0898 (0.3556) FDE: 0.2552 (0.5234)\n",
      "evaluating seq biwi_eth, forecasting frame 000866 to 000877 ADE: 0.2456 (0.3546) FDE: 0.3586 (0.5219)\n",
      "evaluating seq biwi_eth, forecasting frame 000867 to 000878 ADE: 0.3102 (0.3542) FDE: 0.6214 (0.5228)\n",
      "evaluating seq biwi_eth, forecasting frame 000868 to 000879 ADE: 0.1908 (0.3528) FDE: 0.5178 (0.5228)\n",
      "evaluating seq biwi_eth, forecasting frame 000869 to 000880 ADE: 0.1586 (0.3511) FDE: 0.3525 (0.5213)\n",
      "evaluating seq biwi_eth, forecasting frame 000870 to 000881 ADE: 0.3102 (0.3507) FDE: 0.4469 (0.5206)\n",
      "evaluating seq biwi_eth, forecasting frame 000871 to 000882 ADE: 0.4007 (0.3511) FDE: 0.1231 (0.5172)\n",
      "evaluating seq biwi_eth, forecasting frame 000872 to 000883 ADE: 0.3481 (0.3511) FDE: 0.8431 (0.5200)\n",
      "evaluating seq biwi_eth, forecasting frame 000873 to 000884 ADE: 0.4359 (0.3518) FDE: 0.5698 (0.5204)\n",
      "evaluating seq biwi_eth, forecasting frame 000874 to 000885 ADE: 0.3898 (0.3521) FDE: 0.4125 (0.5195)\n",
      "evaluating seq biwi_eth, forecasting frame 000875 to 000886 ADE: 0.3702 (0.3523) FDE: 0.7017 (0.5210)\n",
      "evaluating seq biwi_eth, forecasting frame 000876 to 000887 ADE: 0.4570 (0.3532) FDE: 0.0455 (0.5171)\n",
      "evaluating seq biwi_eth, forecasting frame 000877 to 000888 ADE: 0.5937 (0.3552) FDE: 0.2519 (0.5149)\n",
      "evaluating seq biwi_eth, forecasting frame 000878 to 000889 ADE: 0.2592 (0.3544) FDE: 0.2403 (0.5126)\n",
      "evaluating seq biwi_eth, forecasting frame 000879 to 000890 ADE: 0.6170 (0.3565) FDE: 1.1021 (0.5174)\n",
      "evaluating seq biwi_eth, forecasting frame 000880 to 000891 ADE: 0.3271 (0.3563) FDE: 0.5138 (0.5174)\n",
      "evaluating seq biwi_eth, forecasting frame 000881 to 000892 ADE: 0.2674 (0.3556) FDE: 0.4509 (0.5168)\n",
      "evaluating seq biwi_eth, forecasting frame 000882 to 000893 ADE: 0.6619 (0.3580) FDE: 1.3042 (0.5231)\n",
      "evaluating seq biwi_eth, forecasting frame 000883 to 000894 ADE: 0.9305 (0.3625) FDE: 1.9342 (0.5342)\n",
      "evaluating seq biwi_eth, forecasting frame 000884 to 000895 ADE: 0.8907 (0.3666) FDE: 1.5526 (0.5422)\n",
      "evaluating seq biwi_eth, forecasting frame 000885 to 000896 ADE: 0.4994 (0.3677) FDE: 1.4885 (0.5495)\n",
      "evaluating seq biwi_eth, forecasting frame 000886 to 000897 ADE: 0.4362 (0.3682) FDE: 0.6378 (0.5502)\n",
      "evaluating seq biwi_eth, forecasting frame 000887 to 000898 ADE: 0.1357 (0.3664) FDE: 0.5121 (0.5499)\n",
      "evaluating seq biwi_eth, forecasting frame 000888 to 000899 ADE: 0.2589 (0.3656) FDE: 0.4405 (0.5491)\n",
      "evaluating seq biwi_eth, forecasting frame 000889 to 000900 ADE: 0.3008 (0.3651) FDE: 0.0192 (0.5451)\n",
      "evaluating seq biwi_eth, forecasting frame 000890 to 000901 ADE: 0.3409 (0.3649) FDE: 0.5956 (0.5454)\n",
      "evaluating seq biwi_eth, forecasting frame 000891 to 000902 ADE: 0.1466 (0.3633) FDE: 0.1762 (0.5427)\n",
      "evaluating seq biwi_eth, forecasting frame 000892 to 000903 ADE: 0.2289 (0.3623) FDE: 0.5662 (0.5429)\n",
      "evaluating seq biwi_eth, forecasting frame 000893 to 000904 ADE: 0.3260 (0.3621) FDE: 0.5974 (0.5433)\n",
      "evaluating seq biwi_eth, forecasting frame 000894 to 000905 ADE: 0.2708 (0.3614) FDE: 0.6863 (0.5443)\n",
      "evaluating seq biwi_eth, forecasting frame 000895 to 000906 ADE: 0.2297 (0.3604) FDE: 0.2032 (0.5419)\n",
      "evaluating seq biwi_eth, forecasting frame 000896 to 000907 ADE: 0.3426 (0.3603) FDE: 0.3252 (0.5403)\n",
      "evaluating seq biwi_eth, forecasting frame 000897 to 000908 ADE: 0.7103 (0.3652) FDE: 1.5468 (0.5545)\n",
      "evaluating seq biwi_eth, forecasting frame 000898 to 000909 ADE: 0.7187 (0.3726) FDE: 1.7540 (0.5793)\n",
      "evaluating seq biwi_eth, forecasting frame 000899 to 000910 ADE: 0.9177 (0.3872) FDE: 2.2981 (0.6255)\n",
      "evaluating seq biwi_eth, forecasting frame 000900 to 000911 ADE: 0.7000 (0.3934) FDE: 1.7918 (0.6485)\n",
      "evaluating seq biwi_eth, forecasting frame 000901 to 000912 ADE: 0.2599 (0.3925) FDE: 0.5805 (0.6480)\n",
      "evaluating seq biwi_eth, forecasting frame 000902 to 000913 ADE: 0.2995 (0.3919) FDE: 0.6306 (0.6479)\n",
      "evaluating seq biwi_eth, forecasting frame 000903 to 000914 ADE: 0.3250 (0.3915) FDE: 0.8448 (0.6492)\n",
      "evaluating seq biwi_eth, forecasting frame 000904 to 000915 ADE: 0.3660 (0.3913) FDE: 0.4186 (0.6477)\n",
      "evaluating seq biwi_eth, forecasting frame 000905 to 000916 ADE: 0.2234 (0.3902) FDE: 0.5989 (0.6474)\n",
      "evaluating seq biwi_eth, forecasting frame 000906 to 000917 ADE: 0.3239 (0.3898) FDE: 0.9305 (0.6492)\n",
      "evaluating seq biwi_eth, forecasting frame 000907 to 000918 ADE: 0.4147 (0.3900) FDE: 1.3444 (0.6536)\n",
      "evaluating seq biwi_eth, forecasting frame 000908 to 000919 ADE: 0.5696 (0.3911) FDE: 1.6039 (0.6595)\n",
      "evaluating seq biwi_eth, forecasting frame 000909 to 000920 ADE: 0.4542 (0.3915) FDE: 0.9660 (0.6614)\n",
      "evaluating seq biwi_eth, forecasting frame 000910 to 000921 ADE: 0.5060 (0.3922) FDE: 0.8485 (0.6626)\n",
      "evaluating seq biwi_eth, forecasting frame 000911 to 000922 ADE: 0.6451 (0.3937) FDE: 0.9746 (0.6645)\n",
      "evaluating seq biwi_eth, forecasting frame 000912 to 000923 ADE: 0.5965 (0.3950) FDE: 1.2331 (0.6679)\n",
      "evaluating seq biwi_eth, forecasting frame 000913 to 000924 ADE: 0.6038 (0.3962) FDE: 0.8691 (0.6692)\n",
      "evaluating seq biwi_eth, forecasting frame 000914 to 000925 ADE: 0.7210 (0.3982) FDE: 0.2467 (0.6666)\n",
      "evaluating seq biwi_eth, forecasting frame 000938 to 000949 ADE: 0.1437 (0.3967) FDE: 0.0661 (0.6630)\n",
      "evaluating seq biwi_eth, forecasting frame 000939 to 000950 ADE: 0.4216 (0.3968) FDE: 0.8530 (0.6641)\n",
      "evaluating seq biwi_eth, forecasting frame 000940 to 000951 ADE: 0.3912 (0.3968) FDE: 0.9451 (0.6658)\n",
      "evaluating seq biwi_eth, forecasting frame 000941 to 000952 ADE: 0.6307 (0.3982) FDE: 1.2133 (0.6690)\n",
      "evaluating seq biwi_eth, forecasting frame 000942 to 000953 ADE: 0.3960 (0.3981) FDE: 0.5308 (0.6682)\n",
      "evaluating seq biwi_eth, forecasting frame 000943 to 000954 ADE: 0.3733 (0.3980) FDE: 0.5842 (0.6677)\n",
      "evaluating seq biwi_eth, forecasting frame 000944 to 000955 ADE: 0.9046 (0.4009) FDE: 0.1851 (0.6649)\n",
      "evaluating seq biwi_eth, forecasting frame 000945 to 000956 ADE: 0.3623 (0.4007) FDE: 0.6618 (0.6649)\n",
      "evaluating seq biwi_eth, forecasting frame 000946 to 000957 ADE: 0.3178 (0.4002) FDE: 0.6508 (0.6648)\n",
      "evaluating seq biwi_eth, forecasting frame 000947 to 000958 ADE: 0.5237 (0.4009) FDE: 0.9740 (0.6666)\n",
      "evaluating seq biwi_eth, forecasting frame 000948 to 000959 ADE: 0.3626 (0.4007) FDE: 0.3634 (0.6649)\n",
      "evaluating seq biwi_eth, forecasting frame 000949 to 000960 ADE: 0.2937 (0.4001) FDE: 0.1949 (0.6623)\n",
      "evaluating seq biwi_eth, forecasting frame 000950 to 000961 ADE: 0.3519 (0.3999) FDE: 0.2357 (0.6599)\n",
      "evaluating seq biwi_eth, forecasting frame 000951 to 000962 ADE: 0.4553 (0.4002) FDE: 0.1562 (0.6571)\n",
      "evaluating seq biwi_eth, forecasting frame 000952 to 000963 ADE: 0.8960 (0.4029) FDE: 0.6641 (0.6571)\n",
      "evaluating seq biwi_eth, forecasting frame 000953 to 000964 ADE: 0.2792 (0.4022) FDE: 0.4460 (0.6559)\n",
      "evaluating seq biwi_eth, forecasting frame 000954 to 000965 ADE: 0.3927 (0.4022) FDE: 0.7874 (0.6567)\n",
      "evaluating seq biwi_eth, forecasting frame 000955 to 000966 ADE: 0.2945 (0.4016) FDE: 0.6034 (0.6564)\n",
      "evaluating seq biwi_eth, forecasting frame 000956 to 000967 ADE: 0.2903 (0.4010) FDE: 0.3505 (0.6547)\n",
      "evaluating seq biwi_eth, forecasting frame 000957 to 000968 ADE: 0.2191 (0.4000) FDE: 0.6329 (0.6546)\n",
      "evaluating seq biwi_eth, forecasting frame 000958 to 000969 ADE: 0.1922 (0.3989) FDE: 0.6884 (0.6548)\n",
      "evaluating seq biwi_eth, forecasting frame 000959 to 000970 ADE: 0.3534 (0.3986) FDE: 1.0994 (0.6572)\n",
      "evaluating seq biwi_eth, forecasting frame 000960 to 000971 ADE: 0.3660 (0.3985) FDE: 0.8048 (0.6579)\n",
      "evaluating seq biwi_eth, forecasting frame 000961 to 000972 ADE: 0.3179 (0.3981) FDE: 0.4563 (0.6569)\n",
      "evaluating seq biwi_eth, forecasting frame 000962 to 000973 ADE: 0.4239 (0.3982) FDE: 0.5021 (0.6561)\n",
      "evaluating seq biwi_eth, forecasting frame 000963 to 000974 ADE: 0.5174 (0.3988) FDE: 0.4895 (0.6552)\n",
      "evaluating seq biwi_eth, forecasting frame 000964 to 000975 ADE: 0.6386 (0.4001) FDE: 0.4895 (0.6543)\n",
      "evaluating seq biwi_eth, forecasting frame 000965 to 000976 ADE: 0.7474 (0.4018) FDE: 0.4526 (0.6533)\n",
      "evaluating seq biwi_eth, forecasting frame 000966 to 000977 ADE: 0.4143 (0.4019) FDE: 0.5234 (0.6526)\n",
      "evaluating seq biwi_eth, forecasting frame 000967 to 000978 ADE: 0.4592 (0.4022) FDE: 0.4636 (0.6517)\n",
      "evaluating seq biwi_eth, forecasting frame 000968 to 000979 ADE: 0.4163 (0.4023) FDE: 0.4368 (0.6506)\n",
      "evaluating seq biwi_eth, forecasting frame 000969 to 000980 ADE: 0.8185 (0.4044) FDE: 0.6050 (0.6503)\n",
      "evaluating seq biwi_eth, forecasting frame 000970 to 000981 ADE: 0.4324 (0.4045) FDE: 0.6044 (0.6501)\n",
      "evaluating seq biwi_eth, forecasting frame 000971 to 000982 ADE: 0.2957 (0.4040) FDE: 0.2666 (0.6482)\n",
      "evaluating seq biwi_eth, forecasting frame 000972 to 000983 ADE: 0.2758 (0.4033) FDE: 0.4592 (0.6473)\n",
      "evaluating seq biwi_eth, forecasting frame 000973 to 000984 ADE: 0.1761 (0.4022) FDE: 0.4833 (0.6464)\n",
      "evaluating seq biwi_eth, forecasting frame 000974 to 000985 ADE: 0.2646 (0.4015) FDE: 0.7104 (0.6468)\n",
      "evaluating seq biwi_eth, forecasting frame 000975 to 000986 ADE: 0.3264 (0.4012) FDE: 0.8571 (0.6478)\n",
      "evaluating seq biwi_eth, forecasting frame 000976 to 000987 ADE: 0.5036 (0.4026) FDE: 0.4390 (0.6448)\n",
      "evaluating seq biwi_eth, forecasting frame 000977 to 000988 ADE: 0.6695 (0.4065) FDE: 0.5000 (0.6427)\n",
      "evaluating seq biwi_eth, forecasting frame 000978 to 000989 ADE: 0.6981 (0.4106) FDE: 0.6449 (0.6427)\n",
      "evaluating seq biwi_eth, forecasting frame 000979 to 000990 ADE: 0.8135 (0.4162) FDE: 0.4100 (0.6395)\n",
      "evaluating seq biwi_eth, forecasting frame 000980 to 000991 ADE: 0.8079 (0.4198) FDE: 0.9921 (0.6427)\n",
      "evaluating seq biwi_eth, forecasting frame 000981 to 000992 ADE: 0.6836 (0.4222) FDE: 1.2064 (0.6479)\n",
      "evaluating seq biwi_eth, forecasting frame 000982 to 000993 ADE: 0.6794 (0.4245) FDE: 1.5769 (0.6562)\n",
      "evaluating seq biwi_eth, forecasting frame 000983 to 000994 ADE: 0.7238 (0.4271) FDE: 1.2353 (0.6614)\n",
      "evaluating seq biwi_eth, forecasting frame 000984 to 000995 ADE: 0.5147 (0.4279) FDE: 1.2182 (0.6663)\n",
      "evaluating seq biwi_eth, forecasting frame 000985 to 000996 ADE: 0.7657 (0.4309) FDE: 1.0578 (0.6698)\n",
      "evaluating seq biwi_eth, forecasting frame 000986 to 000997 ADE: 0.9564 (0.4355) FDE: 2.1640 (0.6828)\n",
      "evaluating seq biwi_eth, forecasting frame 000987 to 000998 ADE: 0.9111 (0.4396) FDE: 1.5293 (0.6900)\n",
      "evaluating seq biwi_eth, forecasting frame 001000 to 001011 ADE: 0.5773 (0.4407) FDE: 1.1453 (0.6939)\n",
      "evaluating seq biwi_eth, forecasting frame 001001 to 001012 ADE: 0.5290 (0.4411) FDE: 1.1066 (0.6957)\n",
      "evaluating seq biwi_eth, forecasting frame 001002 to 001013 ADE: 0.6416 (0.4420) FDE: 1.3038 (0.6983)\n",
      "evaluating seq biwi_eth, forecasting frame 001003 to 001014 ADE: 0.9295 (0.4440) FDE: 1.9008 (0.7033)\n",
      "evaluating seq biwi_eth, forecasting frame 001004 to 001015 ADE: 0.6786 (0.4450) FDE: 1.9827 (0.7087)\n",
      "evaluating seq biwi_eth, forecasting frame 001005 to 001016 ADE: 0.5795 (0.4456) FDE: 1.3056 (0.7112)\n",
      "evaluating seq biwi_eth, forecasting frame 001006 to 001017 ADE: 0.6172 (0.4463) FDE: 1.4629 (0.7144)\n",
      "evaluating seq biwi_eth, forecasting frame 001007 to 001018 ADE: 0.6453 (0.4471) FDE: 1.3220 (0.7169)\n",
      "evaluating seq biwi_eth, forecasting frame 001008 to 001019 ADE: 0.4012 (0.4469) FDE: 0.6916 (0.7168)\n",
      "evaluating seq biwi_eth, forecasting frame 001009 to 001020 ADE: 0.5833 (0.4475) FDE: 0.9193 (0.7176)\n",
      "evaluating seq biwi_eth, forecasting frame 001010 to 001021 ADE: 0.9365 (0.4495) FDE: 1.1903 (0.7195)\n",
      "evaluating seq biwi_eth, forecasting frame 001011 to 001022 ADE: 0.2340 (0.4486) FDE: 0.1259 (0.7171)\n",
      "evaluating seq biwi_eth, forecasting frame 001012 to 001023 ADE: 0.7009 (0.4496) FDE: 1.3804 (0.7198)\n",
      "evaluating seq biwi_eth, forecasting frame 001013 to 001024 ADE: 0.7616 (0.4509) FDE: 1.3520 (0.7224)\n",
      "evaluating seq biwi_eth, forecasting frame 001014 to 001025 ADE: 0.3005 (0.4503) FDE: 1.0127 (0.7235)\n",
      "evaluating seq biwi_eth, forecasting frame 001015 to 001026 ADE: 0.3309 (0.4498) FDE: 0.9415 (0.7244)\n",
      "evaluating seq biwi_eth, forecasting frame 001016 to 001027 ADE: 0.3270 (0.4493) FDE: 0.8620 (0.7250)\n",
      "evaluating seq biwi_eth, forecasting frame 001017 to 001028 ADE: 0.3973 (0.4491) FDE: 0.9235 (0.7258)\n",
      "evaluating seq biwi_eth, forecasting frame 001018 to 001029 ADE: 0.4440 (0.4491) FDE: 0.8953 (0.7264)\n",
      "evaluating seq biwi_eth, forecasting frame 001019 to 001030 ADE: 0.3491 (0.4487) FDE: 0.5032 (0.7255)\n",
      "evaluating seq biwi_eth, forecasting frame 001020 to 001031 ADE: 0.5640 (0.4491) FDE: 0.9963 (0.7266)\n",
      "evaluating seq biwi_eth, forecasting frame 001021 to 001032 ADE: 0.6422 (0.4499) FDE: 1.0420 (0.7279)\n",
      "evaluating seq biwi_eth, forecasting frame 001022 to 001033 ADE: 0.5932 (0.4505) FDE: 0.7330 (0.7279)\n",
      "evaluating seq biwi_eth, forecasting frame 001023 to 001034 ADE: 0.2346 (0.4496) FDE: 0.1527 (0.7256)\n",
      "evaluating seq biwi_eth, forecasting frame 001024 to 001035 ADE: 0.6887 (0.4505) FDE: 0.7778 (0.7258)\n",
      "evaluating seq biwi_eth, forecasting frame 001025 to 001036 ADE: 0.1837 (0.4495) FDE: 0.0767 (0.7233)\n",
      "evaluating seq biwi_eth, forecasting frame 001026 to 001037 ADE: 0.7930 (0.4508) FDE: 1.1037 (0.7248)\n",
      "evaluating seq biwi_eth, forecasting frame 001027 to 001038 ADE: 0.2832 (0.4502) FDE: 0.2465 (0.7230)\n",
      "evaluating seq biwi_eth, forecasting frame 001028 to 001039 ADE: 0.2585 (0.4495) FDE: 0.0981 (0.7206)\n",
      "evaluating seq biwi_eth, forecasting frame 001029 to 001040 ADE: 0.2366 (0.4487) FDE: 0.1396 (0.7184)\n",
      "evaluating seq biwi_eth, forecasting frame 001030 to 001041 ADE: 0.3877 (0.4484) FDE: 0.7862 (0.7186)\n",
      "evaluating seq biwi_eth, forecasting frame 001031 to 001042 ADE: 0.1843 (0.4474) FDE: 0.1150 (0.7163)\n",
      "evaluating seq biwi_eth, forecasting frame 001032 to 001043 ADE: 0.2063 (0.4465) FDE: 0.1841 (0.7143)\n",
      "evaluating seq biwi_eth, forecasting frame 001033 to 001044 ADE: 0.4038 (0.4459) FDE: 0.7642 (0.7151)\n",
      "evaluating seq biwi_eth, forecasting frame 001034 to 001045 ADE: 0.5380 (0.4472) FDE: 1.2638 (0.7231)\n",
      "evaluating seq biwi_eth, forecasting frame 001035 to 001046 ADE: 0.4026 (0.4466) FDE: 0.9798 (0.7268)\n",
      "evaluating seq biwi_eth, forecasting frame 001036 to 001047 ADE: 0.9650 (0.4539) FDE: 2.0964 (0.7462)\n",
      "evaluating seq biwi_eth, forecasting frame 001037 to 001048 ADE: 0.2363 (0.4532) FDE: 0.2523 (0.7445)\n",
      "evaluating seq biwi_eth, forecasting frame 001038 to 001049 ADE: 0.4171 (0.4525) FDE: 0.6319 (0.7425)\n",
      "evaluating seq biwi_eth, forecasting frame 001039 to 001050 ADE: 0.4570 (0.4526) FDE: 0.7969 (0.7434)\n",
      "evaluating seq biwi_eth, forecasting frame 001040 to 001051 ADE: 0.6136 (0.4543) FDE: 1.1811 (0.7479)\n",
      "evaluating seq biwi_eth, forecasting frame 001041 to 001052 ADE: 0.6566 (0.4563) FDE: 1.0204 (0.7506)\n",
      "evaluating seq biwi_eth, forecasting frame 001042 to 001053 ADE: 0.6944 (0.4587) FDE: 1.0059 (0.7532)\n",
      "evaluating seq biwi_eth, forecasting frame 001077 to 001088 ADE: 0.5486 (0.4589) FDE: 0.8478 (0.7535)\n",
      "evaluating seq biwi_eth, forecasting frame 001093 to 001104 ADE: 0.2123 (0.4581) FDE: 0.2790 (0.7519)\n",
      "evaluating seq biwi_eth, forecasting frame 001094 to 001105 ADE: 0.1884 (0.4573) FDE: 0.3656 (0.7506)\n",
      "evaluating seq biwi_eth, forecasting frame 001095 to 001106 ADE: 0.2430 (0.4566) FDE: 0.1753 (0.7488)\n",
      "evaluating seq biwi_eth, forecasting frame 001096 to 001107 ADE: 0.2187 (0.4558) FDE: 0.1294 (0.7467)\n",
      "evaluating seq biwi_eth, forecasting frame 001129 to 001140 ADE: 0.9202 (0.4573) FDE: 1.6224 (0.7496)\n",
      "evaluating seq biwi_eth, forecasting frame 001130 to 001141 ADE: 0.2146 (0.4565) FDE: 0.3349 (0.7482)\n",
      "evaluating seq biwi_eth, forecasting frame 001136 to 001147 ADE: 1.0748 (0.4624) FDE: 1.9335 (0.7596)\n",
      "evaluating seq biwi_eth, forecasting frame 001137 to 001148 ADE: 0.9565 (0.4656) FDE: 1.8104 (0.7663)\n",
      "evaluating seq biwi_eth, forecasting frame 001138 to 001149 ADE: 0.4904 (0.4657) FDE: 0.9656 (0.7670)\n",
      "evaluating seq biwi_eth, forecasting frame 001139 to 001150 ADE: 0.7894 (0.4667) FDE: 1.0975 (0.7680)\n",
      "evaluating seq biwi_eth, forecasting frame 001192 to 001203 ADE: 0.4884 (0.4668) FDE: 0.8582 (0.7683)\n",
      "evaluating seq biwi_eth, forecasting frame 001193 to 001204 ADE: 0.6786 (0.4674) FDE: 1.2838 (0.7699)\n",
      "evaluating seq biwi_eth, forecasting frame 001196 to 001207 ADE: 0.6216 (0.4679) FDE: 0.8523 (0.7702)\n",
      "evaluating seq biwi_eth, forecasting frame 001197 to 001208 ADE: 0.4431 (0.4678) FDE: 0.8106 (0.7703)\n",
      "evaluating seq biwi_eth, forecasting frame 001201 to 001212 ADE: 0.4439 (0.4678) FDE: 0.5356 (0.7696)\n",
      "evaluating seq biwi_eth, forecasting frame 001202 to 001213 ADE: 0.5116 (0.4680) FDE: 1.1262 (0.7718)\n",
      "evaluating seq biwi_eth, forecasting frame 001203 to 001214 ADE: 0.6753 (0.4687) FDE: 0.9880 (0.7724)\n",
      "evaluating seq biwi_eth, forecasting frame 001205 to 001216 ADE: 0.7956 (0.4697) FDE: 1.0892 (0.7734)\n",
      "evaluating seq biwi_eth, forecasting frame 001206 to 001217 ADE: 0.6118 (0.4701) FDE: 0.5129 (0.7726)\n",
      "evaluating seq biwi_eth, forecasting frame 001210 to 001221 ADE: 0.6587 (0.4718) FDE: 1.3879 (0.7782)\n",
      "evaluating seq biwi_eth, forecasting frame 001211 to 001222 ADE: 0.9535 (0.4762) FDE: 1.7255 (0.7868)\n",
      "evaluating seq biwi_eth, forecasting frame 001212 to 001223 ADE: 0.2841 (0.4750) FDE: 0.3566 (0.7842)\n",
      "evaluating seq biwi_eth, forecasting frame 001213 to 001224 ADE: 0.1805 (0.4733) FDE: 0.4035 (0.7819)\n",
      "evaluating seq biwi_eth, forecasting frame 001214 to 001225 ADE: 0.3161 (0.4724) FDE: 0.4951 (0.7802)\n",
      "evaluating seq biwi_eth, forecasting frame 001215 to 001226 ADE: 0.1965 (0.4707) FDE: 0.3253 (0.7776)\n",
      "evaluating seq biwi_eth, forecasting frame 001216 to 001227 ADE: 0.0831 (0.4685) FDE: 0.1971 (0.7742)\n",
      "evaluating seq biwi_eth, forecasting frame 001217 to 001228 ADE: 0.1401 (0.4666) FDE: 0.3556 (0.7717)\n",
      "evaluating seq biwi_eth, forecasting frame 001218 to 001229 ADE: 0.2337 (0.4652) FDE: 0.4949 (0.7701)\n",
      "evaluating seq biwi_eth, forecasting frame 001219 to 001230 ADE: 0.1668 (0.4635) FDE: 0.3335 (0.7676)\n",
      "evaluating seq biwi_eth, forecasting frame 001220 to 001231 ADE: 0.1947 (0.4620) FDE: 0.2897 (0.7649)\n",
      "evaluating seq biwi_eth, forecasting frame 001221 to 001232 ADE: 0.1686 (0.4603) FDE: 0.3909 (0.7628)\n",
      "evaluating seq biwi_eth, forecasting frame 001222 to 001233 ADE: 0.1953 (0.4588) FDE: 0.2303 (0.7598)\n",
      "evaluating seq biwi_eth, forecasting frame 001223 to 001234 ADE: 0.1792 (0.4572) FDE: 0.2164 (0.7567)\n",
      "evaluating seq biwi_eth, forecasting frame 001224 to 001235 ADE: 0.2043 (0.4558) FDE: 0.2977 (0.7542)\n",
      "evaluating seq biwi_eth, forecasting frame 001225 to 001236 ADE: 0.1457 (0.4541) FDE: 0.3036 (0.7516)\n",
      "evaluating seq biwi_eth, forecasting frame 001226 to 001237 ADE: 0.1596 (0.4525) FDE: 0.4156 (0.7498)\n",
      "evaluating seq biwi_eth, forecasting frame 001227 to 001238 ADE: 0.2143 (0.4511) FDE: 0.3703 (0.7477)\n",
      "------------------------------ STATS ------------------------------\n",
      "364 ADE: 0.4511\n",
      "364 FDE: 0.7477\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/kuaicv/research/AgentFormer\")   # repo 루트\n",
    "\n",
    "cfg_id    = \"eth_agentformer\"   \n",
    "eval_split = \"test\"\n",
    "epochs_arg = 5                   \n",
    "gpu_id     = -1                    \n",
    "cached     = False\n",
    "cleanup    = False\n",
    "\n",
    "# --------------------\n",
    "# setup (원래 코드와 동일)\n",
    "# --------------------\n",
    "cfg = Config(cfg_id)\n",
    "\n",
    "if epochs_arg is None:\n",
    "    epochs = [cfg.get_last_epoch()]\n",
    "else:\n",
    "    epochs = [int(x) for x in str(epochs_arg).split(\",\")]\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\", index=gpu_id) if gpu_id >= 0 and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "log = open(os.path.join(cfg.log_dir, \"log_test.txt\"), \"w\")\n",
    "\n",
    "print(\"cfg_id:\", cfg_id)\n",
    "print(\"epochs:\", epochs)\n",
    "print(\"device:\", device)\n",
    "print(\"dataset:\", cfg.dataset, \"model_id:\", cfg.get(\"model_id\", \"agentformer\"))\n",
    "print(\"model_path template:\", cfg.model_path)\n",
    "print(\"result_dir:\", cfg.result_dir)\n",
    "\n",
    "\n",
    "for epoch in epochs:\n",
    "    prepare_seed(cfg.seed)\n",
    "\n",
    "    # model\n",
    "    if not cached:\n",
    "        model_id = cfg.get(\"model_id\", \"agentformer\")\n",
    "        print\n",
    "        model = model_dict[model_id](cfg)\n",
    "        model.set_device(device)\n",
    "        model.eval()\n",
    "\n",
    "        if epoch > 0:\n",
    "            cp_path = cfg.model_path % epoch  \n",
    "            print_log(f\"loading model from checkpoint: {cp_path}\", log, display=True)\n",
    "\n",
    "            if not os.path.exists(cp_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Checkpoint not found: {cp_path}\\n\"\n",
    "                    f\"Tip: set cfg_id to the config that actually has checkpoints under results/, \"\n",
    "                    f\"or set epochs_arg to an existing epoch.\\n\"\n",
    "                    f\"Try listing: {os.path.dirname(cp_path)}\"\n",
    "                )\n",
    "\n",
    "            model_cp = torch.load(cp_path, map_location=\"cpu\")\n",
    "            model.load_state_dict(model_cp[\"model_dict\"], strict=False)\n",
    "\n",
    "    # save results and compute metrics\n",
    "    data_splits = [eval_split]\n",
    "\n",
    "    for split in data_splits:\n",
    "        generator = data_generator(cfg, log, split=split, phase=\"testing\")\n",
    "\n",
    "        save_dir = f\"{cfg.result_dir}/epoch_{epoch:04d}/{split}\"\n",
    "        mkdir_if_missing(save_dir)\n",
    "\n",
    "        eval_dir = f\"{save_dir}/samples\"\n",
    "\n",
    "        if not cached:\n",
    "            test_model(generator, save_dir, cfg)\n",
    "\n",
    "        log_file = os.path.join(cfg.log_dir, \"log_eval.txt\")\n",
    "        cmd = f\"python eval.py --dataset {cfg.dataset} --results_dir {eval_dir} --data {split} --log {log_file}\"\n",
    "        print(\"RUN:\", cmd)\n",
    "        subprocess.run(cmd.split(\" \"))\n",
    "\n",
    "        if cleanup:\n",
    "            shutil.rmtree(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
